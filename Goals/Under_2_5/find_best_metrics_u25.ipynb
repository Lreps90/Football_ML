{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T06:07:44.178999Z",
     "start_time": "2025-08-20T06:07:43.991896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# --- Paths ---\n",
    "SRC_DIR  = r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Under_2_5\"\n",
    "DEST_DIR = os.path.join(SRC_DIR, \"best_model_metrics\")\n",
    "PATH_HT_SCORE = os.path.join(SRC_DIR, \"model_file\")  # directory containing .pkl model files\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "# Safety for model-file pruning: set to False to actually delete non-best .pkl files\n",
    "DRY_RUN_PKLS = False\n",
    "\n",
    "# ---------- delete *_FAILED files ----------\n",
    "def is_failed_filename(path: str) -> bool:\n",
    "    base = os.path.basename(path)\n",
    "    name_no_ext, _ = os.path.splitext(base)\n",
    "    return base.endswith(\"_FAILED\") or base.endswith(\"_FAILED.csv\") or name_no_ext.endswith(\"_FAILED\")\n",
    "\n",
    "def purge_failed_files(dirs: list[str]) -> None:\n",
    "    for d in dirs:\n",
    "        pattern = os.path.join(d, \"model_metrics_*.csv\")\n",
    "        for p in glob.glob(pattern):\n",
    "            if os.path.isfile(p) and is_failed_filename(p):\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                    print(f\"✂ Deleted FAILED file: {p}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Could not delete FAILED file {p}: {e}\")\n",
    "\n",
    "# ---------- utils ----------\n",
    "def read_csv_safely(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _normalise_model_path(p: str) -> str | None:\n",
    "    \"\"\"Normalise possibly relative model paths against PATH_HT_SCORE (British spelling ✅).\"\"\"\n",
    "    if not isinstance(p, str) or not p.strip():\n",
    "        return None\n",
    "    p = p.strip().strip('\"').strip(\"'\")\n",
    "    if not os.path.isabs(p):\n",
    "        p = os.path.join(PATH_HT_SCORE, p)\n",
    "    try:\n",
    "        return os.path.realpath(p)\n",
    "    except Exception:\n",
    "        return os.path.abspath(p)\n",
    "\n",
    "def choose_best_model_path(df: pd.DataFrame) -> str | None:\n",
    "    \"\"\"Pick the first non-empty model_pkl in the already-sorted combined DataFrame.\"\"\"\n",
    "    if \"model_pkl\" not in df.columns:\n",
    "        print(\"[WARN] 'model_pkl' column not found in combined CSV — cannot retain any model.\")\n",
    "        return None\n",
    "    for _, row in df.iterrows():\n",
    "        raw = str(row.get(\"model_pkl\", \"\") or \"\").strip()\n",
    "        if raw:\n",
    "            p = _normalise_model_path(raw)\n",
    "            if p:\n",
    "                return p\n",
    "    print(\"[WARN] No non-empty 'model_pkl' found in combined CSV — cannot retain any model.\")\n",
    "    return None\n",
    "\n",
    "def prune_pkls_keep_one(keep_path: str | None) -> None:\n",
    "    \"\"\"Delete all .pkl files in PATH_HT_SCORE except keep_path.\"\"\"\n",
    "    if not os.path.isdir(PATH_HT_SCORE):\n",
    "        print(f\"[WARN] PATH_HT_SCORE does not exist: {PATH_HT_SCORE}\")\n",
    "        return\n",
    "\n",
    "    all_pkls = [\n",
    "        os.path.realpath(os.path.join(PATH_HT_SCORE, name))\n",
    "        for name in os.listdir(PATH_HT_SCORE)\n",
    "        if os.path.isfile(os.path.join(PATH_HT_SCORE, name)) and name.lower().endswith(\".pkl\")\n",
    "    ]\n",
    "\n",
    "    if keep_path:\n",
    "        keep_path = os.path.realpath(keep_path)\n",
    "\n",
    "    to_delete = sorted([p for p in all_pkls if p != keep_path])\n",
    "\n",
    "    print(\"\\n— Pruning model_file directory —\")\n",
    "    print(f\" Found .pkl files: {len(all_pkls)}\")\n",
    "    print(f\" Keeping: {keep_path if keep_path else '(none — no best model_pkl found)'}\")\n",
    "    print(f\" Will delete: {len(to_delete)}\")\n",
    "\n",
    "    if DRY_RUN_PKLS:\n",
    "        for p in to_delete[:20]:\n",
    "            print(f\"   (dry-run) would delete: {p}\")\n",
    "        if len(to_delete) > 20:\n",
    "            print(f\"   ...and {len(to_delete) - 20} more.\")\n",
    "        print(\" Set DRY_RUN_PKLS = False to actually delete.\")\n",
    "        return\n",
    "\n",
    "    for p in to_delete:\n",
    "        try:\n",
    "            os.remove(p)\n",
    "            print(f\" ✂ Deleted: {p}\")\n",
    "        except Exception as e:\n",
    "            print(f\" [WARN] Could not delete {p}: {e}\")\n",
    "\n",
    "def deduplicate_sorted(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    After sorting by val_precision_lcb desc, drop duplicate candidates.\n",
    "    We treat a 'candidate' as (threshold + any present hyper-parameter columns).\n",
    "    \"\"\"\n",
    "    XGB_PARAM_COLS = [\n",
    "        \"n_estimators\", \"max_depth\", \"learning_rate\", \"min_child_weight\",\n",
    "        \"subsample\", \"colsample_bytree\", \"reg_lambda\",\n",
    "    ]\n",
    "    MLP_PARAM_COLS = [\n",
    "        \"hidden_layer_sizes\", \"alpha\", \"learning_rate_init\",\n",
    "        \"batch_size\", \"max_iter\",\n",
    "    ]\n",
    "    key_cols = [c for c in ([\"threshold\"] + XGB_PARAM_COLS + MLP_PARAM_COLS) if c in df.columns]\n",
    "    if key_cols:\n",
    "        return df.drop_duplicates(subset=key_cols, keep=\"first\").reset_index(drop=True)\n",
    "    # Fallback: drop exact duplicates\n",
    "    return df.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # 1) Clean up *_FAILED CSVs first (SRC and DEST)\n",
    "    purge_failed_files([SRC_DIR, DEST_DIR])\n",
    "\n",
    "    # 2) Collect ALL CSVs to merge:\n",
    "    #    - Every CSV in SRC (single-run + COMBINED)\n",
    "    #    - Every COMBINED CSV in DEST (older results)\n",
    "    src_csvs  = [p for p in glob.glob(os.path.join(SRC_DIR,  \"model_metrics_*.csv\")) if not is_failed_filename(p)]\n",
    "    dest_csvs = [p for p in glob.glob(os.path.join(DEST_DIR, \"model_metrics_*COMBINED_*.csv\")) if not is_failed_filename(p)]\n",
    "    merge_inputs = sorted(set(src_csvs + dest_csvs))\n",
    "\n",
    "    if not merge_inputs:\n",
    "        print(\"No CSVs found to merge; nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 3) Load, concat\n",
    "    frames, used_paths = [], []\n",
    "    for p in merge_inputs:\n",
    "        df = read_csv_safely(p)\n",
    "        if df.empty:\n",
    "            print(f\"  [WARN] Empty or unreadable CSV (skipped): {p}\")\n",
    "            continue\n",
    "        frames.append(df)\n",
    "        used_paths.append(p)\n",
    "\n",
    "    if not frames:\n",
    "        print(\"No usable CSV data after reading; nothing to do.\")\n",
    "        return\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True, sort=False)\n",
    "\n",
    "    # 4) Sort by val_precision_lcb (desc) and de-duplicate logical duplicates\n",
    "    if \"val_precision_lcb\" not in combined.columns:\n",
    "        raise KeyError(\"Expected column 'val_precision_lcb' not found in CSVs.\")\n",
    "    combined[\"val_precision_lcb\"] = pd.to_numeric(combined[\"val_precision_lcb\"], errors=\"coerce\")\n",
    "    combined = combined.sort_values(by=[\"val_precision_lcb\"], ascending=False, kind=\"mergesort\").reset_index(drop=True)\n",
    "    combined = deduplicate_sorted(combined)\n",
    "\n",
    "    # 5) Write fresh COMBINED to DEST\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = os.path.join(DEST_DIR, f\"model_metrics_COMBINED_{ts}.csv\")\n",
    "    combined.to_csv(out_path, index=False)\n",
    "    print(f\"✓ Wrote combined CSV ({len(combined)} rows):\\n  {out_path}\")\n",
    "\n",
    "    # 6) Delete ALL CSV inputs used for the merge (both SRC and DEST)\n",
    "    deleted, failed = 0, 0\n",
    "    for p in used_paths:\n",
    "        # never delete the brand-new output we just wrote (not in used_paths anyway)\n",
    "        try:\n",
    "            os.remove(p)\n",
    "            deleted += 1\n",
    "            print(f\" ✂ Deleted merged input CSV: {p}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\" [WARN] Could not delete CSV {p}: {e}\")\n",
    "    print(f\"Input CSV cleanup — deleted: {deleted}, failed: {failed}\")\n",
    "\n",
    "    # 7) Pick the best model_pkl from the top-most row with a non-empty model_pkl\n",
    "    best_model_path = choose_best_model_path(combined)\n",
    "    if best_model_path and not os.path.exists(best_model_path):\n",
    "        print(f\"[WARN] Best model path does not exist on disk (will still keep path): {best_model_path}\")\n",
    "\n",
    "    # 8) Prune PKLs in PATH_HT_SCORE to keep only the best\n",
    "    prune_pkls_keep_one(best_model_path)\n",
    "\n",
    "    print(\"\\nAll done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "6ef8ffd7abe975ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂ Deleted FAILED file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Under_2_5\\model_metrics_20250819_190150_FAILED.csv\n",
      "✂ Deleted FAILED file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Under_2_5\\model_metrics_20250819_211324_FAILED.csv\n",
      "✂ Deleted FAILED file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Under_2_5\\model_metrics_20250820_020937_FAILED.csv\n",
      "✓ Wrote combined CSV (5 rows):\n",
      "  C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Under_2_5\\best_model_metrics\\model_metrics_COMBINED_20250820_070744.csv\n",
      " ✂ Deleted merged input CSV: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Under_2_5\\best_model_metrics\\model_metrics_COMBINED_20250819_112054.csv\n",
      "Input CSV cleanup — deleted: 1, failed: 0\n",
      "\n",
      "— Pruning model_file directory —\n",
      " Found .pkl files: 1\n",
      " Keeping: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Under_2_5\\model_file\\best_model_xgb_calibrated_20250818_175932.pkl\n",
      " Will delete: 0\n",
      "\n",
      "All done.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T06:07:44.224888Z",
     "start_time": "2025-08-20T06:07:44.222062Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "74447c9409864aa0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
