{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:20:47.948209Z",
     "start_time": "2025-05-11T15:20:47.938325Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import function_library as fl\n",
    "from datetime import datetime\n"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:27:41.095620Z",
     "start_time": "2025-05-11T15:27:40.973398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Directory containing the CSV files\n",
    "directory = r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Goals_v3\\best_models_per_league_v4\"\n",
    "\n",
    "# List to collect each top row's data\n",
    "top_rows = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"model_probs_\") and filename.endswith(\".csv\"):\n",
    "        league_name = filename.split(\"_\")[2]  # Extract league name from filename\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            top_row = df.iloc[0]  # Get only the first row\n",
    "            top_rows.append({\n",
    "                'League': league_name,\n",
    "                'Model': top_row['model'],\n",
    "                'Threshold': top_row['threshold'],\n",
    "                'Precision': top_row.get('prec_test'),\n",
    "                'Params': top_row.get('params')\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Combine all results into one DataFrame\n",
    "results_df = pd.DataFrame(top_rows)\n",
    "\n",
    "# Save or display results\n",
    "#results_df.to_csv(\"top_row_model_params.csv\", index=False)\n",
    "\n"
   ],
   "id": "fce1a11317aadeba",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:27:43.825860Z",
     "start_time": "2025-05-11T15:27:43.803661Z"
    }
   },
   "cell_type": "code",
   "source": "results_df",
   "id": "4d8e688853c111c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       League    Model  Threshold  Precision  \\\n",
       "0   ('Aus1',)      MLP        0.4     0.6061   \n",
       "1   ('Aus2',)      MLP        0.1     0.6190   \n",
       "2   ('Aut1',)      MLP        0.4     0.7097   \n",
       "3   ('Bel1',)      MLP        0.2     0.6291   \n",
       "4   ('Bra1',)      MLP        0.6     0.5248   \n",
       "5   ('Bul1',)      MLP        0.5     0.6136   \n",
       "6   ('Chi1',)      MLP        0.1     0.6283   \n",
       "7   ('Czh1',)      MLP        0.2     0.5430   \n",
       "8   ('Den1',)  XGBoost        0.5     0.5862   \n",
       "9   ('Eng1',)      MLP        0.7     0.6667   \n",
       "10  ('Eng2',)      MLP        0.7     0.5714   \n",
       "11  ('Eng3',)      MLP        0.3     0.5057   \n",
       "12  ('Eng4',)      MLP        0.5     0.5287   \n",
       "13  ('Fra1',)      MLP        0.4     0.5403   \n",
       "14  ('Fra2',)  XGBoost        0.2     0.4677   \n",
       "15  ('Ger1',)      MLP        0.5     0.6270   \n",
       "16  ('Ger2',)      MLP        0.4     0.5852   \n",
       "17  ('Ger3',)  XGBoost        0.5     0.5887   \n",
       "18  ('Gre1',)      MLP        0.6     0.7115   \n",
       "19  ('Hun1',)      MLP        0.6     0.6765   \n",
       "20  ('Ice1',)      MLP        0.5     0.6699   \n",
       "21  ('Ire1',)      MLP        0.5     0.5465   \n",
       "22  ('Isr1',)  XGBoost        0.1     0.5273   \n",
       "23  ('Ita1',)  XGBoost        0.3     0.5674   \n",
       "24  ('Ita2',)      MLP        0.4     0.5395   \n",
       "25  ('Jap1',)      MLP        0.8     0.6383   \n",
       "26  ('Jap2',)      MLP        0.7     0.6129   \n",
       "27  ('Kor1',)      MLP        0.3     0.5198   \n",
       "28  ('Mex1',)      MLP        0.3     0.5447   \n",
       "29  ('Ned1',)  XGBoost        0.3     0.6488   \n",
       "30  ('Nor1',)      MLP        0.3     0.6212   \n",
       "31  ('Pol1',)      MLP        0.5     0.5532   \n",
       "32  ('Por1',)      MLP        0.2     0.5174   \n",
       "33  ('Rom1',)      MLP        0.4     0.5000   \n",
       "34  ('Sco1',)      MLP        0.7     0.7000   \n",
       "35  ('Sco2',)      MLP        0.3     0.5182   \n",
       "36  ('Slk1',)  XGBoost        0.7     0.6027   \n",
       "37  ('Slo1',)      MLP        0.6     0.6292   \n",
       "38  ('Spa1',)      MLP        0.2     0.5203   \n",
       "39  ('Spa2',)  XGBoost        0.8     0.4667   \n",
       "40  ('Swe1',)  XGBoost        0.8     0.5868   \n",
       "41  ('Swi1',)      MLP        0.3     0.6553   \n",
       "42  ('Swi2',)  XGBoost        0.8     0.6800   \n",
       "43  ('Tur1',)  XGBoost        0.2     0.5831   \n",
       "44  ('Tur2',)  XGBoost        0.8     0.5484   \n",
       "45  ('USA1',)  XGBoost        0.7     0.6667   \n",
       "\n",
       "                                               Params  \n",
       "0   {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "1   {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "2   {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "3   {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "4   {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "5   {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "6   {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "7   {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "8   {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "9   {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "10  {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "11  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "12  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "13  {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "14  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "15  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "16  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "17  {'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...  \n",
       "18  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "19  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "20  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "21  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "22  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "23  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "24  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "25  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "26  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "27  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "28  {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "29  {'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...  \n",
       "30  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "31  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "32  {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "33  {'alpha': 0.01, 'early_stopping': True, 'hidde...  \n",
       "34  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "35  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "36  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "37  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "38  {'alpha': 0.001, 'early_stopping': True, 'hidd...  \n",
       "39  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "40  {'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...  \n",
       "41  {'alpha': 0.0001, 'early_stopping': True, 'hid...  \n",
       "42  {'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...  \n",
       "43  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "44  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  \n",
       "45  {'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>League</th>\n",
       "      <th>Model</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('Aus1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6061</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('Aus2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6190</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('Aut1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7097</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('Bel1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.6291</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('Bra1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5248</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('Bul1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6136</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('Chi1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.6283</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('Czh1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5430</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('Den1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5862</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('Eng1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('Eng2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5714</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('Eng3',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5057</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('Eng4',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5287</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>('Fra1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>('Fra2',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4677</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>('Ger1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6270</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>('Ger2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('Ger3',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5887</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>('Gre1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7115</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>('Hun1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>('Ice1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6699</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>('Ire1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5465</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>('Isr1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5273</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>('Ita1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5674</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>('Ita2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5395</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>('Jap1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6383</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>('Jap2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6129</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>('Kor1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>('Mex1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5447</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>('Ned1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6488</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>('Nor1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>('Pol1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5532</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>('Por1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5174</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>('Rom1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'alpha': 0.01, 'early_stopping': True, 'hidde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>('Sco1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>('Sco2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>('Slk1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>('Slo1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6292</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>('Spa1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5203</td>\n",
       "      <td>{'alpha': 0.001, 'early_stopping': True, 'hidd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>('Spa2',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4667</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>('Swe1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>('Swi1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6553</td>\n",
       "      <td>{'alpha': 0.0001, 'early_stopping': True, 'hid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>('Swi2',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.1, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>('Tur1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5831</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>('Tur2',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5484</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>('USA1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'gamma': 0.0, 'learn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:23:48.224346Z",
     "start_time": "2025-05-11T15:23:48.200958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = [\n",
    "    # 'Unnamed: 0',\n",
    "    # 'country',\n",
    "    # 'season',\n",
    "    # 'date',\n",
    "    # 'ko_time',\n",
    "    'round',\n",
    "    # 'home_team',\n",
    "    # 'away_team',\n",
    "    # 'home_goals_ft',\n",
    "    # 'away_goals_ft',\n",
    "    # 'home_goals_ht',\n",
    "    # 'away_goals_ht',\n",
    "    'home_team_place_total',\n",
    "    'home_team_place_home',\n",
    "    'away_team_place_total',\n",
    "    'away_team_place_away',\n",
    "    'home_odds',\n",
    "    'draw_odds',\n",
    "    'away_odds',\n",
    "    'over_25_odds',\n",
    "    'under_25_odds',\n",
    "    'elo_home',\n",
    "    'elo_away',\n",
    "    'form_home',\n",
    "    'form_away',\n",
    "    # 'shots_home',\n",
    "    # 'shots_home_1h',\n",
    "    # 'shots_home_2h',\n",
    "    # 'shots_away',\n",
    "    # 'shots_away_1h',\n",
    "    # 'shots_away_2h',\n",
    "    # 'shots_on_target_home',\n",
    "    # 'shots_on_target_home_1h',\n",
    "    # 'shots_on_target_home_2h',\n",
    "    # 'shots_on_target_away',\n",
    "    # 'shots_on_target_away_1h',\n",
    "    # 'shots_on_target_away_2h',\n",
    "    # 'corners_home',\n",
    "    # 'corners_home_1h',\n",
    "    # 'corners_home_2h',\n",
    "    # 'corners_away',\n",
    "    # 'corners_away_1h',\n",
    "    # 'corners_away_2h',\n",
    "    # 'fouls_home',\n",
    "    # 'fouls_home_1h',\n",
    "    # 'fouls_home_2h',\n",
    "    # 'fouls_away',\n",
    "    # 'fouls_away_1h',\n",
    "    # 'fouls_away_2h',\n",
    "    # 'yellow_cards_home',\n",
    "    # 'yellow_cards_home_1h',\n",
    "    # 'yellow_cards_home_2h',\n",
    "    # 'yellow_cards_away',\n",
    "    # 'yellow_cards_away_1h',\n",
    "    # 'yellow_cards_away_2h',\n",
    "    # 'possession_home',\n",
    "    # 'possession_home_1h',\n",
    "    # 'possession_home_2h',\n",
    "    # 'possession_away',\n",
    "    # 'possession_away_1h',\n",
    "    # 'possession_away_2h',\n",
    "    # 'goals_scored_total_home',\n",
    "    # 'goals_conceded_total_home',\n",
    "    # 'goals_scored_total_away',\n",
    "    # 'goals_conceded_total_away',\n",
    "    # 'points_home',\n",
    "    # 'points_away',\n",
    "    # 'is_home_x',\n",
    "    'home_Overall_Rolling_GoalsScored_Mean',\n",
    "    'home_Overall_Rolling_GoalsScored_Std',\n",
    "    'home_Overall_Rolling_GoalsScored_Mean_Short',\n",
    "    'home_Overall_Momentum_GoalsScored',\n",
    "    'home_Overall_Trend_Slope_GoalsScored',\n",
    "    'home_Overall_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'home_Overall_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'home_Overall_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'home_Overall_Momentum_FirstHalfGoalsScored',\n",
    "    'home_Overall_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'home_Overall_Rolling_Shots_Mean',\n",
    "    'home_Overall_Rolling_Shots_Std',\n",
    "    'home_Overall_Rolling_Shots_Mean_Short',\n",
    "    'home_Overall_Momentum_Shots',\n",
    "    'home_Overall_Trend_Slope_Shots',\n",
    "    'home_Overall_Rolling_Shots_1h_Mean',\n",
    "    'home_Overall_Rolling_Shots_1h_Std',\n",
    "    'home_Overall_Rolling_Shots_1h_Mean_Short',\n",
    "    'home_Overall_Momentum_Shots_1h',\n",
    "    'home_Overall_Trend_Slope_Shots_1h',\n",
    "    'home_Overall_Rolling_Corners_Mean',\n",
    "    'home_Overall_Rolling_Corners_Std',\n",
    "    'home_Overall_Rolling_Corners_Mean_Short',\n",
    "    'home_Overall_Momentum_Corners',\n",
    "    'home_Overall_Trend_Slope_Corners',\n",
    "    'home_Overall_Rolling_Corners_1h_Mean',\n",
    "    'home_Overall_Rolling_Corners_1h_Std',\n",
    "    'home_Overall_Rolling_Corners_1h_Mean_Short',\n",
    "    'home_Overall_Momentum_Corners_1h',\n",
    "    'home_Overall_Trend_Slope_Corners_1h',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_Mean',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_Std',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'home_Overall_Momentum_ShotsOnTarget',\n",
    "    'home_Overall_Trend_Slope_ShotsOnTarget',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'home_Overall_Momentum_ShotsOnTarget_1h',\n",
    "    'home_Overall_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'home_Rolling_GoalsScored_Mean',\n",
    "    'home_Rolling_GoalsScored_Std',\n",
    "    'home_Rolling_GoalsScored_Mean_Short',\n",
    "    'home_Momentum_GoalsScored',\n",
    "    'home_Trend_Slope_GoalsScored',\n",
    "    'home_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'home_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'home_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'home_Momentum_FirstHalfGoalsScored',\n",
    "    'home_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'home_Rolling_Shots_Mean',\n",
    "    'home_Rolling_Shots_Std',\n",
    "    'home_Rolling_Shots_Mean_Short',\n",
    "    'home_Momentum_Shots',\n",
    "    'home_Trend_Slope_Shots',\n",
    "    'home_Rolling_Shots_1h_Mean',\n",
    "    'home_Rolling_Shots_1h_Std',\n",
    "    'home_Rolling_Shots_1h_Mean_Short',\n",
    "    'home_Momentum_Shots_1h',\n",
    "    'home_Trend_Slope_Shots_1h',\n",
    "    'home_Rolling_Corners_Mean',\n",
    "    'home_Rolling_Corners_Std',\n",
    "    'home_Rolling_Corners_Mean_Short',\n",
    "    'home_Momentum_Corners',\n",
    "    'home_Trend_Slope_Corners',\n",
    "    'home_Rolling_Corners_1h_Mean',\n",
    "    'home_Rolling_Corners_1h_Std',\n",
    "    'home_Rolling_Corners_1h_Mean_Short',\n",
    "    'home_Momentum_Corners_1h',\n",
    "    'home_Trend_Slope_Corners_1h',\n",
    "    'home_Rolling_ShotsOnTarget_Mean',\n",
    "    'home_Rolling_ShotsOnTarget_Std',\n",
    "    'home_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'home_Momentum_ShotsOnTarget',\n",
    "    'home_Trend_Slope_ShotsOnTarget',\n",
    "    'home_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'home_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'home_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'home_Momentum_ShotsOnTarget_1h',\n",
    "    'home_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'home_Overall_Percent_Over_1.5',\n",
    "    'home_Overall_Rolling5_Percent_Over_1.5',\n",
    "    'home_Percent_Over_1.5',\n",
    "    'home_Rolling5_Percent_Over_1.5',\n",
    "    'home_Overall_Percent_Over_2.5',\n",
    "    'home_Overall_Rolling5_Percent_Over_2.5',\n",
    "    'home_Percent_Over_2.5',\n",
    "    'home_Rolling5_Percent_Over_2.5',\n",
    "    'home_Overall_Percent_Over_3.5',\n",
    "    'home_Overall_Rolling5_Percent_Over_3.5',\n",
    "    'home_Percent_Over_3.5',\n",
    "    'home_Rolling5_Percent_Over_3.5',\n",
    "    'home_TeamPct_Over_0.5',\n",
    "    'home_TeamPct_Over_1.5',\n",
    "    'home_TeamPct_Over_2.5',\n",
    "    'home_TeamPct_Over_3.5',\n",
    "    'home_CornersPct_Over_3.5',\n",
    "    'home_CornersRolling5Pct_Over_3.5',\n",
    "    'home_CornersPct_Over_4.5',\n",
    "    'home_CornersRolling5Pct_Over_4.5',\n",
    "    'home_CornersPct_Over_5.5',\n",
    "    'home_CornersRolling5Pct_Over_5.5',\n",
    "    'home_CornersPct_Over_6.5',\n",
    "    'home_CornersRolling5Pct_Over_6.5',\n",
    "    'home_SeasonPct_Over_9.5',\n",
    "    'home_Rolling5Pct_Over_9.5',\n",
    "    'home_SeasonPct_Over_10.5',\n",
    "    'home_Rolling5Pct_Over_10.5',\n",
    "    'home_SeasonPct_Over_11.5',\n",
    "    'home_Rolling5Pct_Over_11.5',\n",
    "    # 'is_home_y',\n",
    "    'away_Overall_Rolling_GoalsScored_Mean',\n",
    "    'away_Overall_Rolling_GoalsScored_Std',\n",
    "    'away_Overall_Rolling_GoalsScored_Mean_Short',\n",
    "    'away_Overall_Momentum_GoalsScored',\n",
    "    'away_Overall_Trend_Slope_GoalsScored',\n",
    "    'away_Overall_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'away_Overall_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'away_Overall_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'away_Overall_Momentum_FirstHalfGoalsScored',\n",
    "    'away_Overall_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'away_Overall_Rolling_Shots_Mean',\n",
    "    'away_Overall_Rolling_Shots_Std',\n",
    "    'away_Overall_Rolling_Shots_Mean_Short',\n",
    "    'away_Overall_Momentum_Shots',\n",
    "    'away_Overall_Trend_Slope_Shots',\n",
    "    'away_Overall_Rolling_Shots_1h_Mean',\n",
    "    'away_Overall_Rolling_Shots_1h_Std',\n",
    "    'away_Overall_Rolling_Shots_1h_Mean_Short',\n",
    "    'away_Overall_Momentum_Shots_1h',\n",
    "    'away_Overall_Trend_Slope_Shots_1h',\n",
    "    'away_Overall_Rolling_Corners_Mean',\n",
    "    'away_Overall_Rolling_Corners_Std',\n",
    "    'away_Overall_Rolling_Corners_Mean_Short',\n",
    "    'away_Overall_Momentum_Corners',\n",
    "    'away_Overall_Trend_Slope_Corners',\n",
    "    'away_Overall_Rolling_Corners_1h_Mean',\n",
    "    'away_Overall_Rolling_Corners_1h_Std',\n",
    "    'away_Overall_Rolling_Corners_1h_Mean_Short',\n",
    "    'away_Overall_Momentum_Corners_1h',\n",
    "    'away_Overall_Trend_Slope_Corners_1h',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_Mean',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_Std',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'away_Overall_Momentum_ShotsOnTarget',\n",
    "    'away_Overall_Trend_Slope_ShotsOnTarget',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'away_Overall_Momentum_ShotsOnTarget_1h',\n",
    "    'away_Overall_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'away_Rolling_GoalsScored_Mean',\n",
    "    'away_Rolling_GoalsScored_Std',\n",
    "    'away_Rolling_GoalsScored_Mean_Short',\n",
    "    'away_Momentum_GoalsScored',\n",
    "    'away_Trend_Slope_GoalsScored',\n",
    "    'away_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'away_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'away_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'away_Momentum_FirstHalfGoalsScored',\n",
    "    'away_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'away_Rolling_Shots_Mean',\n",
    "    'away_Rolling_Shots_Std',\n",
    "    'away_Rolling_Shots_Mean_Short',\n",
    "    'away_Momentum_Shots',\n",
    "    'away_Trend_Slope_Shots',\n",
    "    'away_Rolling_Shots_1h_Mean',\n",
    "    'away_Rolling_Shots_1h_Std',\n",
    "    'away_Rolling_Shots_1h_Mean_Short',\n",
    "    'away_Momentum_Shots_1h',\n",
    "    'away_Trend_Slope_Shots_1h',\n",
    "    'away_Rolling_Corners_Mean',\n",
    "    'away_Rolling_Corners_Std',\n",
    "    'away_Rolling_Corners_Mean_Short',\n",
    "    'away_Momentum_Corners',\n",
    "    'away_Trend_Slope_Corners',\n",
    "    'away_Rolling_Corners_1h_Mean',\n",
    "    'away_Rolling_Corners_1h_Std',\n",
    "    'away_Rolling_Corners_1h_Mean_Short',\n",
    "    'away_Momentum_Corners_1h',\n",
    "    'away_Trend_Slope_Corners_1h',\n",
    "    'away_Rolling_ShotsOnTarget_Mean',\n",
    "    'away_Rolling_ShotsOnTarget_Std',\n",
    "    'away_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'away_Momentum_ShotsOnTarget',\n",
    "    'away_Trend_Slope_ShotsOnTarget',\n",
    "    'away_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'away_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'away_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'away_Momentum_ShotsOnTarget_1h',\n",
    "    'away_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'away_Overall_Percent_Over_1.5',\n",
    "    'away_Overall_Rolling5_Percent_Over_1.5',\n",
    "    'away_Percent_Over_1.5',\n",
    "    'away_Rolling5_Percent_Over_1.5',\n",
    "    'away_Overall_Percent_Over_2.5',\n",
    "    'away_Overall_Rolling5_Percent_Over_2.5',\n",
    "    'away_Percent_Over_2.5',\n",
    "    'away_Rolling5_Percent_Over_2.5',\n",
    "    'away_Overall_Percent_Over_3.5',\n",
    "    'away_Overall_Rolling5_Percent_Over_3.5',\n",
    "    'away_Percent_Over_3.5',\n",
    "    'away_Rolling5_Percent_Over_3.5',\n",
    "    'away_TeamPct_Over_0.5',\n",
    "    'away_TeamPct_Over_1.5',\n",
    "    'away_TeamPct_Over_2.5',\n",
    "    'away_TeamPct_Over_3.5',\n",
    "    'away_CornersPct_Over_3.5',\n",
    "    'away_CornersRolling5Pct_Over_3.5',\n",
    "    'away_CornersPct_Over_4.5',\n",
    "    'away_CornersRolling5Pct_Over_4.5',\n",
    "    'away_CornersPct_Over_5.5',\n",
    "    'away_CornersRolling5Pct_Over_5.5',\n",
    "    'away_CornersPct_Over_6.5',\n",
    "    'away_CornersRolling5Pct_Over_6.5',\n",
    "    'away_SeasonPct_Over_9.5',\n",
    "    'away_Rolling5Pct_Over_9.5',\n",
    "    'away_SeasonPct_Over_10.5',\n",
    "    'away_Rolling5Pct_Over_10.5',\n",
    "    'away_SeasonPct_Over_11.5',\n",
    "    'away_Rolling5Pct_Over_11.5'\n",
    "]"
   ],
   "id": "c91fc6401176381e",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:24:00.112180Z",
     "start_time": "2025-05-11T15:23:51.878645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_prepared_data(file_path):\n",
    "    data = pd.read_csv(file_path,\n",
    "                       low_memory=False)\n",
    "    # Convert 'date' column to datetime object\n",
    "    data['date'] = pd.to_datetime(data['date'], format=\"%Y-%m-%d\", errors='coerce')\n",
    "    data = data.sort_values(by='date')\n",
    "\n",
    "    # Convert today's date to a pandas Timestamp for compatibility.\n",
    "    today = pd.Timestamp(datetime.today().date())\n",
    "    data = data[data['date'] <= today]\n",
    "\n",
    "    # Clean up and finalise the match-level DataFrame\n",
    "    data.dropna(inplace=True)\n",
    "    data['total_goals'] = data['home_goals_ft'] + data['away_goals_ft']\n",
    "    data['target'] = data['total_goals'].apply(lambda x: 1 if x > 2.5 else 0)\n",
    "    return data\n",
    "matches = pre_prepared_data(r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\engineered_master_data_ALL_2017+.csv\")\n",
    "\n",
    "# Process each league separately\n",
    "leagues = matches[['country']].drop_duplicates().apply(tuple, axis=1)"
   ],
   "id": "d7394273217bc600",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:24:00.152048Z",
     "start_time": "2025-05-11T15:24:00.125382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "matches_filtered = matches[matches['country']=='Aus1']\n",
    "data =matches_filtered.copy()\n",
    "#matches_filtered"
   ],
   "id": "e6ab350b9509433e",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:52:37.988158Z",
     "start_time": "2025-05-11T15:52:37.965584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def replicate_run_from_csv_row(data, features, row, stake=1.0):\n",
    "#     \"\"\"\n",
    "#     Simulate the exact value-bet backtest, including scaling, for a saved model configuration.\n",
    "#\n",
    "#     - Splits data chronologically: first 80% train, last 20% test\n",
    "#     - Builds pipeline: StandardScaler -> base model\n",
    "#     - Fits and calibrates via Platt scaling\n",
    "#     - On test, computes model-implied odds = 1/prob\n",
    "#     - Identifies value bets where model_odds < market odds\n",
    "#     - Stakes `stake` units per bet, computes profit\n",
    "#     - Returns metrics: num_bets, total_profit, roi, strike_rate, p-value\n",
    "#     \"\"\"\n",
    "#     from sklearn.pipeline import Pipeline\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     from sklearn.calibration import CalibratedClassifierCV\n",
    "#\n",
    "#     # Extract model info\n",
    "#     import ast\n",
    "#     model_name = row['Model']\n",
    "#     params = ast.literal_eval(row['Params']) if isinstance(row['Params'], str) else row['Params']\n",
    "#     odds_feature = 'over_25_odds'\n",
    "#\n",
    "#     # Chronological split\n",
    "#     split_idx = int(len(data) * 0.8)\n",
    "#     train_df = data.iloc[:split_idx]\n",
    "#     test_df = data.iloc[split_idx:].copy()\n",
    "#     X_train, y_train = train_df[features], train_df['target']\n",
    "#     X_test, y_test = test_df[features], test_df['target']\n",
    "#\n",
    "#     # Instantiate base model\n",
    "#     if model_name.lower().startswith('mlp'):\n",
    "#         from sklearn.neural_network import MLPClassifier\n",
    "#         base = MLPClassifier(random_state=42, **params)\n",
    "#     elif model_name.lower().startswith('xgboost'):\n",
    "#         from xgboost import XGBClassifier\n",
    "#         base = XGBClassifier(random_state=42, eval_metric='logloss', **params)\n",
    "#     elif 'forest' in model_name.lower():\n",
    "#         from sklearn.ensemble import RandomForestClassifier\n",
    "#         base = RandomForestClassifier(random_state=42, **params)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "#\n",
    "#     # Build and fit scaling pipeline\n",
    "#     pipe = Pipeline([\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('model',  base)\n",
    "#     ])\n",
    "#     pipe.fit(X_train, y_train)\n",
    "#\n",
    "#     # Calibrate probabilities\n",
    "#     calibrated = CalibratedClassifierCV(pipe, method='sigmoid', cv='prefit')\n",
    "#     calibrated.fit(X_train, y_train)\n",
    "#\n",
    "#     # Predict and compute odds\n",
    "#     probs = calibrated.predict_proba(X_test)[:, 1]\n",
    "#     test_df['model_odds'] = 1/(probs + 1e-12)\n",
    "#\n",
    "#     # Identify value bets\n",
    "#     bets = test_df.loc[test_df['model_odds'] < test_df[odds_feature]].copy()\n",
    "#     # Compute profit per bet\n",
    "#     bets['profit'] = bets.apply(\n",
    "#         lambda r: (r[odds_feature] - 1) * stake if r['target'] == 1 else -stake,\n",
    "#         axis=1\n",
    "#     )\n",
    "#\n",
    "#     # Aggregate metrics\n",
    "#     num_bets = len(bets)\n",
    "#     total_profit = bets['profit'].sum()\n",
    "#     roi = total_profit / num_bets if num_bets > 0 else 0\n",
    "#     strike_rate = bets['target'].sum() / num_bets if num_bets > 0 else 0\n",
    "#     from scipy import stats\n",
    "#     pvalue = None\n",
    "#     if num_bets > 1:\n",
    "#         _, pvalue = stats.ttest_1samp(bets['profit'], 0)\n",
    "#\n",
    "#     return {\n",
    "#         'model_name': model_name,\n",
    "#         'model_params': params,\n",
    "#         'num_bets': num_bets,\n",
    "#         'total_profit': round(total_profit, 4),\n",
    "#         'roi': round(roi, 4),\n",
    "#         'strike_rate': round(strike_rate, 4),\n",
    "#         'pvalue': round(pvalue, 4) if pvalue is not None else None\n",
    "#     }\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "def replicate_exact_row(data, features, row,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42,\n",
    "                        apply_calibration=True,\n",
    "                        min_samples=25):\n",
    "    \"\"\"\n",
    "    Re-run the exact train/test split, model, threshold & P/L logic\n",
    "    from one CSV row (with columns 'Model', 'Params', 'threshold', etc.).\n",
    "    \"\"\"\n",
    "    # 1. Exact same random split\n",
    "    train_df, test_df = train_test_split(\n",
    "        data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_tr, y_tr = train_df[features], train_df['target']\n",
    "    X_te, y_te = test_df[features], test_df['target']\n",
    "    odds_te = test_df['over_25_odds'].values\n",
    "\n",
    "    # 2. Build the model from row['Model'] & row['Params']\n",
    "    model_name = row['Model']\n",
    "    params = (ast.literal_eval(row['Params'])\n",
    "              if isinstance(row['Params'], str)\n",
    "              else row['Params'])\n",
    "\n",
    "    if model_name.lower().startswith('mlp'):\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        base = MLPClassifier(random_state=random_state, **params)\n",
    "    elif model_name.lower().startswith('xgboost'):\n",
    "        from xgboost import XGBClassifier\n",
    "        base = XGBClassifier(random_state=random_state,\n",
    "                             eval_metric='logloss',\n",
    "                             **params)\n",
    "    elif 'forest' in model_name.lower():\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        base = RandomForestClassifier(random_state=random_state,\n",
    "                                      **params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name!r}\")\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model',  base)\n",
    "    ])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "\n",
    "    clf = CalibratedClassifierCV(pipe, method='sigmoid', cv='prefit') \\\n",
    "          if apply_calibration else pipe\n",
    "    if apply_calibration:\n",
    "        clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # 3. Predict and apply the *same* threshold\n",
    "    probs = clf.predict_proba(X_te)[:, 1]\n",
    "    t = float(row['Threshold'])\n",
    "    mask = probs >= t\n",
    "    n_bets = int(mask.sum())\n",
    "\n",
    "    if n_bets < min_samples:\n",
    "        raise RuntimeError(\n",
    "            f\"Threshold {t:.2f} yields only {n_bets} bets (<{min_samples})\"\n",
    "        )\n",
    "\n",
    "    # 4. Compute profit exactly as before\n",
    "    profits = np.where(\n",
    "        mask & (y_te.values == 1),\n",
    "        odds_te - 1,\n",
    "        np.where(mask, -1, 0)\n",
    "    )\n",
    "    total_pl = profits.sum()\n",
    "    roi = total_pl / n_bets\n",
    "\n",
    "\n",
    "    return {\n",
    "        'bets_test': n_bets,\n",
    "        'pl_test':   round(total_pl, 2),\n",
    "        'roi_test':  round(roi, 4),\n",
    "    }\n"
   ],
   "id": "63e697756e8b41a2",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T15:52:43.141741Z",
     "start_time": "2025-05-11T15:52:42.873057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "row = results_df.iloc[0]\n",
    "metrics = replicate_exact_row(data, features, row)\n",
    "print(metrics)"
   ],
   "id": "2a811571cbe416ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bets_test': 165, 'pl_test': np.float64(5.35), 'roi_test': np.float64(0.0324)}\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T11:40:02.226239Z",
     "start_time": "2025-05-10T11:40:02.209802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import ast\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def train_and_save_model(data, features, row, output_dir):\n",
    "    \"\"\"\n",
    "    Trains and saves a calibrated model pipeline for a given league configuration.\n",
    "\n",
    "    - Splits data 80% train / 20% test chronologically\n",
    "    - Instantiates model from row['Model'] and row['Params']\n",
    "    - Fits model on train set and calibrates probabilities\n",
    "    - Saves the calibrated pipeline to a pickle named by league and model\n",
    "\n",
    "    Returns the path to the saved pickle.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Extract configuration\n",
    "    raw_league = row.get('League', 'unknown')\n",
    "    # Normalize league format: handle tuples like ('Arg1',)\n",
    "    if isinstance(raw_league, (tuple, list)):\n",
    "        league = raw_league[0]\n",
    "    else:\n",
    "        try:\n",
    "            parsed = ast.literal_eval(raw_league) if isinstance(raw_league, str) else raw_league\n",
    "            league = parsed[0] if isinstance(parsed, (tuple, list)) else str(parsed)\n",
    "        except Exception:\n",
    "            league = str(raw_league)\n",
    "\n",
    "    model_name = row['Model']\n",
    "    params     = ast.literal_eval(row['Params']) if isinstance(row['Params'], str) else row['Params']\n",
    "    fname      = f\"{league}_{model_name}.pkl\"\n",
    "    path       = os.path.join(output_dir, fname)\n",
    "\n",
    "    # Chronological split\n",
    "    split_idx = int(len(data) * 0.8)\n",
    "    train_df = data.iloc[:split_idx]\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df['target']\n",
    "\n",
    "    # Instantiate base estimator\n",
    "    if model_name.lower().startswith('mlp'):\n",
    "        base = MLPClassifier(random_state=42, **params)\n",
    "    elif model_name.lower().startswith('xgboost'):\n",
    "        base = XGBClassifier(random_state=42, eval_metric='logloss', **params)\n",
    "    elif 'forest' in model_name.lower():\n",
    "        base = RandomForestClassifier(random_state=42, **params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # Build full pipeline: scaler -> base -> calibrator\n",
    "    pipeline = ImbPipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', base)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    calibrated = CalibratedClassifierCV(pipeline, method='sigmoid', cv='prefit')\n",
    "    calibrated.fit(X_train, y_train)\n",
    "\n",
    "    # Save pipeline\n",
    "    joblib.dump(calibrated, path)\n",
    "    print(f\"Saved calibrated model pipeline to {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def train_and_save_all_league_models(data, features, results_df, output_dir):\n",
    "    \"\"\"\n",
    "    Iterates over each row in results_df, filters data for that league, trains and saves the model.\n",
    "\n",
    "    Returns a list of filepaths for all saved models.\n",
    "    \"\"\"\n",
    "    saved_paths = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        raw_league = row.get('League', 'unknown')\n",
    "        # Normalize league format\n",
    "        if isinstance(raw_league, (tuple, list)):\n",
    "            league = raw_league[0]\n",
    "        else:\n",
    "            try:\n",
    "                parsed = ast.literal_eval(raw_league) if isinstance(raw_league, str) else raw_league\n",
    "                league = parsed[0] if isinstance(parsed, (tuple, list)) else str(parsed)\n",
    "            except Exception:\n",
    "                league = str(raw_league)\n",
    "\n",
    "        league_df = data[data['country'] == league]\n",
    "        if league_df.empty:\n",
    "            print(f\"No data for league {league}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        path = train_and_save_model(league_df, features, row, output_dir)\n",
    "        saved_paths.append(path)\n",
    "    return saved_paths\n"
   ],
   "id": "4c48feccaa62e638",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T11:42:02.553148Z",
     "start_time": "2025-05-10T11:40:02.301812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_directory = r\"path\\to\\save\\models\"\n",
    "saved_model_files = train_and_save_all_league_models(matches, features, results_df, output_directory)\n",
    "#print(\"Saved model files:\", saved_model_files)"
   ],
   "id": "7e32eb553596be98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved calibrated model pipeline to path\\to\\save\\models\\Arg1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Aus1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Aus2_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Bra1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Bul1_MLP.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Chl1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Cro1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Czh1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Den1_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Eng1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Eng2_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Eng3_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Fra1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Fra2_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ger1_MLP.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ger2_MLP.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ger3_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Gre1_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Hun1_MLP.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ice1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ire1_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Isr1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ita1_MLP.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ita2_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Jap1_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Jap2_MLP.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Kor1_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Ned1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Pol1_MLP.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Por1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Rom1_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Sco1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Sco2_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Slk1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Slo1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Spa1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Spa2_RandomForest.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Swe1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Swi1_XGBoost.pkl\n",
      "Saved calibrated model pipeline to path\\to\\save\\models\\Tur1_XGBoost.pkl\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T11:42:02.664480Z",
     "start_time": "2025-05-10T11:42:02.655835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_test_model(pkl_path, matches, features, odds_feature='over_25_odds', stake=1.0):\n",
    "    \"\"\"\n",
    "    Loads a saved calibrated model pipeline from `pkl_path`, applies it to the last 20% of `data`,\n",
    "    simulates the same value-bet logic, and prints summary metrics to verify correctness.\n",
    "\n",
    "    Args:\n",
    "      pkl_path (str): path to the saved .pkl model file\n",
    "      data (pd.DataFrame): full dataset including features, 'target', and odds_feature\n",
    "      features (list): list of feature column names\n",
    "      odds_feature (str): column name for market odds\n",
    "      stake (float): units staked per value bet\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame of individual bet results and a summary dict of metrics\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    calibrated = joblib.load(pkl_path)\n",
    "    # Prepare test split\n",
    "    split_idx = int(len(data) * 0.8)\n",
    "    test_df = data.iloc[split_idx:].copy()\n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df['target']\n",
    "    # Predict probabilities and implied odds\n",
    "    proba = calibrated.predict_proba(X_test)[:,1]\n",
    "    test_df['model_prob'] = proba\n",
    "    test_df['model_odds'] = 1/(proba + 1e-12)\n",
    "    # Identify value bets\n",
    "    bets = test_df.loc[test_df['model_odds'] < test_df[odds_feature]].copy()\n",
    "    # Simulate profit\n",
    "    bets['profit'] = bets.apply(\n",
    "        lambda r: (r[odds_feature] - 1) * stake if r['target']==1 else -stake,\n",
    "        axis=1\n",
    "    )\n",
    "    # Compute summary metrics\n",
    "    num_bets = len(bets)\n",
    "    total_profit = bets['profit'].sum()\n",
    "    roi = total_profit / num_bets if num_bets>0 else 0\n",
    "    strike_rate = bets['target'].sum() / num_bets if num_bets>0 else 0\n",
    "    from scipy import stats\n",
    "    pvalue = None\n",
    "    if num_bets > 1:\n",
    "        _, pvalue = stats.ttest_1samp(bets['profit'], 0)\n",
    "    summary = {\n",
    "        'model_file': pkl_path,\n",
    "        'num_bets': num_bets,\n",
    "        'total_profit': round(total_profit,4),\n",
    "        'roi': round(roi,4),\n",
    "        'strike_rate': round(strike_rate,4),\n",
    "        'pvalue': round(pvalue,4) if pvalue is not None else None\n",
    "    }\n",
    "    print(\"Test Summary:\")\n",
    "    for k,v in summary.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    return bets, summary\n",
    "\n"
   ],
   "id": "97ae462ba431b60c",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T11:42:02.729917Z",
     "start_time": "2025-05-10T11:42:02.677307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bets_df, metrics = load_and_test_model(\n",
    "    r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Goals_v3\\path\\to\\save\\models\\Aus1_XGBoost.pkl\",\n",
    "    matches_filtered,\n",
    "    features\n",
    ")\n"
   ],
   "id": "dca5827788ad7e58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary:\n",
      "  model_file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\Goals_v3\\path\\to\\save\\models\\Aus1_XGBoost.pkl\n",
      "  num_bets: 95\n",
      "  total_profit: 5.62\n",
      "  roi: 0.0592\n",
      "  strike_rate: 0.5895\n",
      "  pvalue: 0.5266\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T11:42:02.826800Z",
     "start_time": "2025-05-10T11:42:02.805780Z"
    }
   },
   "cell_type": "code",
   "source": "results_df",
   "id": "c5c7917c0d2c6ba4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       League         Model  Strike Rate  \\\n",
       "0   ('Arg1',)       XGBoost       0.4262   \n",
       "1   ('Aus1',)       XGBoost       0.5895   \n",
       "2   ('Aus2',)       XGBoost       0.6846   \n",
       "3   ('Bra1',)       XGBoost       0.5738   \n",
       "4   ('Bul1',)           MLP       0.5205   \n",
       "5   ('Chl1',)       XGBoost       0.5536   \n",
       "6   ('Cro1',)       XGBoost       0.5577   \n",
       "7   ('Czh1',)       XGBoost       0.5810   \n",
       "8   ('Den1',)  RandomForest       0.5962   \n",
       "9   ('Eng1',)       XGBoost       0.6708   \n",
       "10  ('Eng2',)  RandomForest       0.5697   \n",
       "11  ('Eng3',)  RandomForest       0.5522   \n",
       "12  ('Fra1',)       XGBoost       0.5921   \n",
       "13  ('Fra2',)  RandomForest       0.5361   \n",
       "14  ('Ger1',)           MLP       0.6370   \n",
       "15  ('Ger2',)           MLP       0.6626   \n",
       "16  ('Ger3',)       XGBoost       0.5933   \n",
       "17  ('Gre1',)  RandomForest       0.5870   \n",
       "18  ('Hun1',)           MLP       0.6000   \n",
       "19  ('Ice1',)       XGBoost       0.7103   \n",
       "20  ('Ire1',)  RandomForest       0.6250   \n",
       "21  ('Isr1',)       XGBoost       0.5761   \n",
       "22  ('Ita1',)           MLP       0.5796   \n",
       "23  ('Ita2',)  RandomForest       0.5800   \n",
       "24  ('Jap1',)  RandomForest       0.5319   \n",
       "25  ('Jap2',)           MLP       0.5390   \n",
       "26  ('Kor1',)  RandomForest       0.5579   \n",
       "27  ('Ned1',)       XGBoost       0.6851   \n",
       "28  ('Pol1',)           MLP       0.5655   \n",
       "29  ('Por1',)       XGBoost       0.6592   \n",
       "30  ('Rom1',)  RandomForest       0.5246   \n",
       "31  ('Sco1',)       XGBoost       0.6114   \n",
       "32  ('Sco2',)  RandomForest       0.5844   \n",
       "33  ('Slk1',)       XGBoost       0.5938   \n",
       "34  ('Slo1',)       XGBoost       0.6346   \n",
       "35  ('Spa1',)       XGBoost       0.5502   \n",
       "36  ('Spa2',)  RandomForest       0.4922   \n",
       "37  ('Swe1',)       XGBoost       0.6577   \n",
       "38  ('Swi1',)       XGBoost       0.6074   \n",
       "39  ('Tur1',)       XGBoost       0.6259   \n",
       "\n",
       "                                               Params  \n",
       "0   {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "1   {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "2   {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "3   {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "4   {'alpha': 0.0001, 'hidden_layer_sizes': (100, ...  \n",
       "5   {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "6   {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "7   {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "8   {'class_weight': 'balanced', 'max_depth': 10, ...  \n",
       "9   {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "10  {'class_weight': 'balanced', 'max_depth': None...  \n",
       "11  {'class_weight': 'balanced', 'max_depth': 10, ...  \n",
       "12  {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "13  {'class_weight': 'balanced', 'max_depth': 10, ...  \n",
       "14  {'alpha': 0.001, 'hidden_layer_sizes': (100, 5...  \n",
       "15  {'alpha': 0.0001, 'hidden_layer_sizes': (100,)...  \n",
       "16  {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "17  {'class_weight': 'balanced', 'max_depth': 10, ...  \n",
       "18  {'alpha': 0.001, 'hidden_layer_sizes': (100, 5...  \n",
       "19  {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "20  {'class_weight': 'balanced', 'max_depth': 10, ...  \n",
       "21  {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "22  {'alpha': 0.001, 'hidden_layer_sizes': (100,),...  \n",
       "23  {'class_weight': 'balanced', 'max_depth': None...  \n",
       "24  {'class_weight': 'balanced', 'max_depth': None...  \n",
       "25  {'alpha': 0.001, 'hidden_layer_sizes': (100, 5...  \n",
       "26  {'class_weight': 'balanced', 'max_depth': 10, ...  \n",
       "27  {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "28  {'alpha': 0.0001, 'hidden_layer_sizes': (100, ...  \n",
       "29  {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "30  {'class_weight': 'balanced', 'max_depth': None...  \n",
       "31  {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "32  {'class_weight': 'balanced', 'max_depth': None...  \n",
       "33  {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "34  {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "35  {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "36  {'class_weight': 'balanced', 'max_depth': 10, ...  \n",
       "37  {'colsample_bytree': 0.8, 'learning_rate': 0.0...  \n",
       "38  {'colsample_bytree': 0.7, 'learning_rate': 0.0...  \n",
       "39  {'colsample_bytree': 0.8, 'learning_rate': 0.0...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>League</th>\n",
       "      <th>Model</th>\n",
       "      <th>Strike Rate</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('Arg1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('Aus1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5895</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('Aus2',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6846</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('Bra1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5738</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('Bul1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>{'alpha': 0.0001, 'hidden_layer_sizes': (100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('Chl1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5536</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('Cro1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('Czh1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>('Den1',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>('Eng1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6708</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>('Eng2',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5697</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': None...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>('Eng3',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>('Fra1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5921</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>('Fra2',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5361</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>('Ger1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6370</td>\n",
       "      <td>{'alpha': 0.001, 'hidden_layer_sizes': (100, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>('Ger2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6626</td>\n",
       "      <td>{'alpha': 0.0001, 'hidden_layer_sizes': (100,)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>('Ger3',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5933</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('Gre1',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5870</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>('Hun1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>{'alpha': 0.001, 'hidden_layer_sizes': (100, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>('Ice1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.7103</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>('Ire1',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>('Isr1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5761</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>('Ita1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>{'alpha': 0.001, 'hidden_layer_sizes': (100,),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>('Ita2',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': None...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>('Jap1',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5319</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': None...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>('Jap2',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>{'alpha': 0.001, 'hidden_layer_sizes': (100, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>('Kor1',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5579</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>('Ned1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6851</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>('Pol1',)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>{'alpha': 0.0001, 'hidden_layer_sizes': (100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>('Por1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6592</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>('Rom1',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': None...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>('Sco1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6114</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>('Sco2',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5844</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': None...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>('Slk1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5938</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>('Slo1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6346</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>('Spa1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.5502</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>('Spa2',)</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.4922</td>\n",
       "      <td>{'class_weight': 'balanced', 'max_depth': 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>('Swe1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6577</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>('Swi1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6074</td>\n",
       "      <td>{'colsample_bytree': 0.7, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>('Tur1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.6259</td>\n",
       "      <td>{'colsample_bytree': 0.8, 'learning_rate': 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
