{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-15T11:35:37.825108Z",
     "start_time": "2025-08-15T11:35:35.951425Z"
    }
   },
   "source": [
    "import ast\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, matthews_corrcoef,\n",
    "    accuracy_score, confusion_matrix\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:01.078738Z",
     "start_time": "2025-05-16T08:56:01.065331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sort_files_by_precision(directory: str, inplace: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Reads all CSV files in `directory`, sorts each by the 'Precision_Test' column\n",
    "    in descending order, and writes the sorted DataFrame back.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the folder containing CSVs.\n",
    "        inplace (bool):\n",
    "            - If True, overwrite each original file.\n",
    "            - If False, write to new files prefixed with 'sorted_'.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(directory, \"*.csv\")\n",
    "    for filepath in glob.glob(pattern):\n",
    "        df = pd.read_csv(filepath)\n",
    "        if 'Precision_Test' not in df.columns:\n",
    "            print(f\"Skipping {os.path.basename(filepath)}: no 'Precision_Test' column\")\n",
    "            continue\n",
    "\n",
    "        # sort so highest Precision_Test is at the top\n",
    "        df_sorted = df.sort_values(by='Precision_Test', ascending=False)\n",
    "\n",
    "        if inplace:\n",
    "            df_sorted.to_csv(filepath, index=False)\n",
    "            print(f\"✔ Sorted (in‐place): {os.path.basename(filepath)}\")\n",
    "        else:\n",
    "            dirname, filename = os.path.split(filepath)\n",
    "            new_filename = f\"sorted_{filename}\"\n",
    "            df_sorted.to_csv(os.path.join(dirname, new_filename), index=False)\n",
    "            print(f\"✔ Written sorted file: {new_filename}\")\n",
    "\n",
    "\n",
    "directory = r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\best_models_by_ht_scoreline\"\n",
    "sort_files_by_precision(directory, inplace=True)\n"
   ],
   "id": "cdf062636a7bcb43",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:01.439691Z",
     "start_time": "2025-05-16T08:56:01.257705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Directory containing the CSV files\n",
    "directory = r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\"\n",
    "\n",
    "# List to collect each top row's data\n",
    "top_rows = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"model_metrics_(\") and filename.endswith(\".csv\"):\n",
    "        ht_score = filename.split(\"_\")[2]  # Extract league name from filename\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            top_row = df.iloc[0]  # Get only the first row\n",
    "            top_rows.append({\n",
    "                'HT_Score': ht_score,\n",
    "                'Model': top_row['Model'],\n",
    "                'SMOTE': top_row.get('SMOTE'),\n",
    "                'Precision_Test': top_row.get('Precision_Test'),\n",
    "                'Precision_Test/Train_Ratio': top_row.get('Precision_Test/Train_Ratio'),\n",
    "                'Probability_Threshold': top_row.get('Probability_Threshold'),\n",
    "                'Params': top_row.get('Params')\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Combine all results into one DataFrame\n",
    "results_df = pd.DataFrame(top_rows)\n",
    "\n",
    "# Save or display results\n",
    "#results_df.to_csv(\"top_row_model_params.csv\", index=False)\n",
    "\n"
   ],
   "id": "fce1a11317aadeba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing model_metrics_('0-2',)_20250516_003952.csv: No columns to parse from file\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:01.604500Z",
     "start_time": "2025-05-16T08:56:01.573509Z"
    }
   },
   "cell_type": "code",
   "source": "results_df",
   "id": "4d8e688853c111c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   HT_Score    Model  SMOTE  Precision_Test  Precision_Test/Train_Ratio  \\\n",
       "0  ('0-0',)  XGBoost    NaN          0.8513                      0.9226   \n",
       "1  ('0-1',)  XGBoost   0.82          0.8525                      0.9492   \n",
       "2  ('1-0',)  XGBoost   0.33          0.8525                      0.9049   \n",
       "3  ('1-1',)  XGBoost   0.60          0.8502                      0.9383   \n",
       "4  ('1-2',)  XGBoost   0.90          0.8531                      0.9380   \n",
       "5  ('2-0',)  XGBoost   0.61          0.8508                      0.9627   \n",
       "6  ('2-1',)  XGBoost   0.47          0.8522                      0.9110   \n",
       "7  ('3-0',)  XGBoost   0.68          0.8528                      0.9066   \n",
       "\n",
       "   Probability_Threshold                                             Params  \n",
       "0                   0.72  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "1                   0.77  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "2                   0.78  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "3                   0.76  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "4                   0.65  {'classifier__colsample_bytree': 0.8, 'classif...  \n",
       "5                   0.71  {'classifier__colsample_bytree': 0.8, 'classif...  \n",
       "6                   0.73  {'classifier__colsample_bytree': 0.8, 'classif...  \n",
       "7                   0.62  {'classifier__colsample_bytree': 0.7, 'classif...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT_Score</th>\n",
       "      <th>Model</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>Precision_Test</th>\n",
       "      <th>Precision_Test/Train_Ratio</th>\n",
       "      <th>Probability_Threshold</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('0-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('0-1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>0.77</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('1-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>0.9049</td>\n",
       "      <td>0.78</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('1-1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.76</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('1-2',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('2-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>0.9627</td>\n",
       "      <td>0.71</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('2-1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.9110</td>\n",
       "      <td>0.73</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('3-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.8528</td>\n",
       "      <td>0.9066</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:01.802163Z",
     "start_time": "2025-05-16T08:56:01.785342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = [\n",
    "    # 'Unnamed: 0',\n",
    "    # 'country',\n",
    "    # 'season',\n",
    "    # 'date',\n",
    "    # 'ko_time',\n",
    "    'round',\n",
    "    # 'home_team',\n",
    "    # 'away_team',\n",
    "    # 'home_goals_ft',\n",
    "    # 'away_goals_ft',\n",
    "    # 'home_goals_ht',\n",
    "    # 'away_goals_ht',\n",
    "    'home_team_place_total',\n",
    "    'home_team_place_home',\n",
    "    'away_team_place_total',\n",
    "    'away_team_place_away',\n",
    "    'home_odds',\n",
    "    'draw_odds',\n",
    "    'away_odds',\n",
    "    'over_25_odds',\n",
    "    'under_25_odds',\n",
    "    'elo_home',\n",
    "    'elo_away',\n",
    "    'form_home',\n",
    "    'form_away',\n",
    "    # 'shots_home',\n",
    "    # 'shots_home_1h',\n",
    "    # 'shots_home_2h',\n",
    "    # 'shots_away',\n",
    "    # 'shots_away_1h',\n",
    "    # 'shots_away_2h',\n",
    "    # 'shots_on_target_home',\n",
    "    # 'shots_on_target_home_1h',\n",
    "    # 'shots_on_target_home_2h',\n",
    "    # 'shots_on_target_away',\n",
    "    # 'shots_on_target_away_1h',\n",
    "    # 'shots_on_target_away_2h',\n",
    "    # 'corners_home',\n",
    "    # 'corners_home_1h',\n",
    "    # 'corners_home_2h',\n",
    "    # 'corners_away',\n",
    "    # 'corners_away_1h',\n",
    "    # 'corners_away_2h',\n",
    "    # 'fouls_home',\n",
    "    # 'fouls_home_1h',\n",
    "    # 'fouls_home_2h',\n",
    "    # 'fouls_away',\n",
    "    # 'fouls_away_1h',\n",
    "    # 'fouls_away_2h',\n",
    "    # 'yellow_cards_home',\n",
    "    # 'yellow_cards_home_1h',\n",
    "    # 'yellow_cards_home_2h',\n",
    "    # 'yellow_cards_away',\n",
    "    # 'yellow_cards_away_1h',\n",
    "    # 'yellow_cards_away_2h',\n",
    "    # 'possession_home',\n",
    "    # 'possession_home_1h',\n",
    "    # 'possession_home_2h',\n",
    "    # 'possession_away',\n",
    "    # 'possession_away_1h',\n",
    "    # 'possession_away_2h',\n",
    "    # 'goals_scored_total_home',\n",
    "    # 'goals_conceded_total_home',\n",
    "    # 'goals_scored_total_away',\n",
    "    # 'goals_conceded_total_away',\n",
    "    # 'points_home',\n",
    "    # 'points_away',\n",
    "    # 'is_home_x',\n",
    "    'home_Overall_Rolling_GoalsScored_Mean',\n",
    "    'home_Overall_Rolling_GoalsScored_Std',\n",
    "    'home_Overall_Rolling_GoalsScored_Mean_Short',\n",
    "    'home_Overall_Momentum_GoalsScored',\n",
    "    'home_Overall_Trend_Slope_GoalsScored',\n",
    "    'home_Overall_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'home_Overall_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'home_Overall_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'home_Overall_Momentum_FirstHalfGoalsScored',\n",
    "    'home_Overall_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'home_Overall_Rolling_Shots_Mean',\n",
    "    'home_Overall_Rolling_Shots_Std',\n",
    "    'home_Overall_Rolling_Shots_Mean_Short',\n",
    "    'home_Overall_Momentum_Shots',\n",
    "    'home_Overall_Trend_Slope_Shots',\n",
    "    'home_Overall_Rolling_Shots_1h_Mean',\n",
    "    'home_Overall_Rolling_Shots_1h_Std',\n",
    "    'home_Overall_Rolling_Shots_1h_Mean_Short',\n",
    "    'home_Overall_Momentum_Shots_1h',\n",
    "    'home_Overall_Trend_Slope_Shots_1h',\n",
    "    'home_Overall_Rolling_Corners_Mean',\n",
    "    'home_Overall_Rolling_Corners_Std',\n",
    "    'home_Overall_Rolling_Corners_Mean_Short',\n",
    "    'home_Overall_Momentum_Corners',\n",
    "    'home_Overall_Trend_Slope_Corners',\n",
    "    'home_Overall_Rolling_Corners_1h_Mean',\n",
    "    'home_Overall_Rolling_Corners_1h_Std',\n",
    "    'home_Overall_Rolling_Corners_1h_Mean_Short',\n",
    "    'home_Overall_Momentum_Corners_1h',\n",
    "    'home_Overall_Trend_Slope_Corners_1h',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_Mean',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_Std',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'home_Overall_Momentum_ShotsOnTarget',\n",
    "    'home_Overall_Trend_Slope_ShotsOnTarget',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'home_Overall_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'home_Overall_Momentum_ShotsOnTarget_1h',\n",
    "    'home_Overall_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'home_Rolling_GoalsScored_Mean',\n",
    "    'home_Rolling_GoalsScored_Std',\n",
    "    'home_Rolling_GoalsScored_Mean_Short',\n",
    "    'home_Momentum_GoalsScored',\n",
    "    'home_Trend_Slope_GoalsScored',\n",
    "    'home_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'home_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'home_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'home_Momentum_FirstHalfGoalsScored',\n",
    "    'home_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'home_Rolling_Shots_Mean',\n",
    "    'home_Rolling_Shots_Std',\n",
    "    'home_Rolling_Shots_Mean_Short',\n",
    "    'home_Momentum_Shots',\n",
    "    'home_Trend_Slope_Shots',\n",
    "    'home_Rolling_Shots_1h_Mean',\n",
    "    'home_Rolling_Shots_1h_Std',\n",
    "    'home_Rolling_Shots_1h_Mean_Short',\n",
    "    'home_Momentum_Shots_1h',\n",
    "    'home_Trend_Slope_Shots_1h',\n",
    "    'home_Rolling_Corners_Mean',\n",
    "    'home_Rolling_Corners_Std',\n",
    "    'home_Rolling_Corners_Mean_Short',\n",
    "    'home_Momentum_Corners',\n",
    "    'home_Trend_Slope_Corners',\n",
    "    'home_Rolling_Corners_1h_Mean',\n",
    "    'home_Rolling_Corners_1h_Std',\n",
    "    'home_Rolling_Corners_1h_Mean_Short',\n",
    "    'home_Momentum_Corners_1h',\n",
    "    'home_Trend_Slope_Corners_1h',\n",
    "    'home_Rolling_ShotsOnTarget_Mean',\n",
    "    'home_Rolling_ShotsOnTarget_Std',\n",
    "    'home_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'home_Momentum_ShotsOnTarget',\n",
    "    'home_Trend_Slope_ShotsOnTarget',\n",
    "    'home_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'home_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'home_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'home_Momentum_ShotsOnTarget_1h',\n",
    "    'home_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'home_Overall_Percent_Over_1.5',\n",
    "    'home_Overall_Rolling5_Percent_Over_1.5',\n",
    "    'home_Percent_Over_1.5',\n",
    "    'home_Rolling5_Percent_Over_1.5',\n",
    "    'home_Overall_Percent_Over_2.5',\n",
    "    'home_Overall_Rolling5_Percent_Over_2.5',\n",
    "    'home_Percent_Over_2.5',\n",
    "    'home_Rolling5_Percent_Over_2.5',\n",
    "    'home_Overall_Percent_Over_3.5',\n",
    "    'home_Overall_Rolling5_Percent_Over_3.5',\n",
    "    'home_Percent_Over_3.5',\n",
    "    'home_Rolling5_Percent_Over_3.5',\n",
    "    'home_TeamPct_Over_0.5',\n",
    "    'home_TeamPct_Over_1.5',\n",
    "    'home_TeamPct_Over_2.5',\n",
    "    'home_TeamPct_Over_3.5',\n",
    "    'home_CornersPct_Over_3.5',\n",
    "    'home_CornersRolling5Pct_Over_3.5',\n",
    "    'home_CornersPct_Over_4.5',\n",
    "    'home_CornersRolling5Pct_Over_4.5',\n",
    "    'home_CornersPct_Over_5.5',\n",
    "    'home_CornersRolling5Pct_Over_5.5',\n",
    "    'home_CornersPct_Over_6.5',\n",
    "    'home_CornersRolling5Pct_Over_6.5',\n",
    "    'home_SeasonPct_Over_9.5',\n",
    "    'home_Rolling5Pct_Over_9.5',\n",
    "    'home_SeasonPct_Over_10.5',\n",
    "    'home_Rolling5Pct_Over_10.5',\n",
    "    'home_SeasonPct_Over_11.5',\n",
    "    'home_Rolling5Pct_Over_11.5',\n",
    "    # 'is_home_y',\n",
    "    'away_Overall_Rolling_GoalsScored_Mean',\n",
    "    'away_Overall_Rolling_GoalsScored_Std',\n",
    "    'away_Overall_Rolling_GoalsScored_Mean_Short',\n",
    "    'away_Overall_Momentum_GoalsScored',\n",
    "    'away_Overall_Trend_Slope_GoalsScored',\n",
    "    'away_Overall_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'away_Overall_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'away_Overall_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'away_Overall_Momentum_FirstHalfGoalsScored',\n",
    "    'away_Overall_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'away_Overall_Rolling_Shots_Mean',\n",
    "    'away_Overall_Rolling_Shots_Std',\n",
    "    'away_Overall_Rolling_Shots_Mean_Short',\n",
    "    'away_Overall_Momentum_Shots',\n",
    "    'away_Overall_Trend_Slope_Shots',\n",
    "    'away_Overall_Rolling_Shots_1h_Mean',\n",
    "    'away_Overall_Rolling_Shots_1h_Std',\n",
    "    'away_Overall_Rolling_Shots_1h_Mean_Short',\n",
    "    'away_Overall_Momentum_Shots_1h',\n",
    "    'away_Overall_Trend_Slope_Shots_1h',\n",
    "    'away_Overall_Rolling_Corners_Mean',\n",
    "    'away_Overall_Rolling_Corners_Std',\n",
    "    'away_Overall_Rolling_Corners_Mean_Short',\n",
    "    'away_Overall_Momentum_Corners',\n",
    "    'away_Overall_Trend_Slope_Corners',\n",
    "    'away_Overall_Rolling_Corners_1h_Mean',\n",
    "    'away_Overall_Rolling_Corners_1h_Std',\n",
    "    'away_Overall_Rolling_Corners_1h_Mean_Short',\n",
    "    'away_Overall_Momentum_Corners_1h',\n",
    "    'away_Overall_Trend_Slope_Corners_1h',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_Mean',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_Std',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'away_Overall_Momentum_ShotsOnTarget',\n",
    "    'away_Overall_Trend_Slope_ShotsOnTarget',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'away_Overall_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'away_Overall_Momentum_ShotsOnTarget_1h',\n",
    "    'away_Overall_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'away_Rolling_GoalsScored_Mean',\n",
    "    'away_Rolling_GoalsScored_Std',\n",
    "    'away_Rolling_GoalsScored_Mean_Short',\n",
    "    'away_Momentum_GoalsScored',\n",
    "    'away_Trend_Slope_GoalsScored',\n",
    "    'away_Rolling_FirstHalfGoalsScored_Mean',\n",
    "    'away_Rolling_FirstHalfGoalsScored_Std',\n",
    "    'away_Rolling_FirstHalfGoalsScored_Mean_Short',\n",
    "    'away_Momentum_FirstHalfGoalsScored',\n",
    "    'away_Trend_Slope_FirstHalfGoalsScored',\n",
    "    'away_Rolling_Shots_Mean',\n",
    "    'away_Rolling_Shots_Std',\n",
    "    'away_Rolling_Shots_Mean_Short',\n",
    "    'away_Momentum_Shots',\n",
    "    'away_Trend_Slope_Shots',\n",
    "    'away_Rolling_Shots_1h_Mean',\n",
    "    'away_Rolling_Shots_1h_Std',\n",
    "    'away_Rolling_Shots_1h_Mean_Short',\n",
    "    'away_Momentum_Shots_1h',\n",
    "    'away_Trend_Slope_Shots_1h',\n",
    "    'away_Rolling_Corners_Mean',\n",
    "    'away_Rolling_Corners_Std',\n",
    "    'away_Rolling_Corners_Mean_Short',\n",
    "    'away_Momentum_Corners',\n",
    "    'away_Trend_Slope_Corners',\n",
    "    'away_Rolling_Corners_1h_Mean',\n",
    "    'away_Rolling_Corners_1h_Std',\n",
    "    'away_Rolling_Corners_1h_Mean_Short',\n",
    "    'away_Momentum_Corners_1h',\n",
    "    'away_Trend_Slope_Corners_1h',\n",
    "    'away_Rolling_ShotsOnTarget_Mean',\n",
    "    'away_Rolling_ShotsOnTarget_Std',\n",
    "    'away_Rolling_ShotsOnTarget_Mean_Short',\n",
    "    'away_Momentum_ShotsOnTarget',\n",
    "    'away_Trend_Slope_ShotsOnTarget',\n",
    "    'away_Rolling_ShotsOnTarget_1h_Mean',\n",
    "    'away_Rolling_ShotsOnTarget_1h_Std',\n",
    "    'away_Rolling_ShotsOnTarget_1h_Mean_Short',\n",
    "    'away_Momentum_ShotsOnTarget_1h',\n",
    "    'away_Trend_Slope_ShotsOnTarget_1h',\n",
    "    'away_Overall_Percent_Over_1.5',\n",
    "    'away_Overall_Rolling5_Percent_Over_1.5',\n",
    "    'away_Percent_Over_1.5',\n",
    "    'away_Rolling5_Percent_Over_1.5',\n",
    "    'away_Overall_Percent_Over_2.5',\n",
    "    'away_Overall_Rolling5_Percent_Over_2.5',\n",
    "    'away_Percent_Over_2.5',\n",
    "    'away_Rolling5_Percent_Over_2.5',\n",
    "    'away_Overall_Percent_Over_3.5',\n",
    "    'away_Overall_Rolling5_Percent_Over_3.5',\n",
    "    'away_Percent_Over_3.5',\n",
    "    'away_Rolling5_Percent_Over_3.5',\n",
    "    'away_TeamPct_Over_0.5',\n",
    "    'away_TeamPct_Over_1.5',\n",
    "    'away_TeamPct_Over_2.5',\n",
    "    'away_TeamPct_Over_3.5',\n",
    "    'away_CornersPct_Over_3.5',\n",
    "    'away_CornersRolling5Pct_Over_3.5',\n",
    "    'away_CornersPct_Over_4.5',\n",
    "    'away_CornersRolling5Pct_Over_4.5',\n",
    "    'away_CornersPct_Over_5.5',\n",
    "    'away_CornersRolling5Pct_Over_5.5',\n",
    "    'away_CornersPct_Over_6.5',\n",
    "    'away_CornersRolling5Pct_Over_6.5',\n",
    "    'away_SeasonPct_Over_9.5',\n",
    "    'away_Rolling5Pct_Over_9.5',\n",
    "    'away_SeasonPct_Over_10.5',\n",
    "    'away_Rolling5Pct_Over_10.5',\n",
    "    'away_SeasonPct_Over_11.5',\n",
    "    'away_Rolling5Pct_Over_11.5'\n",
    "]"
   ],
   "id": "c91fc6401176381e",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:00:51.592040Z",
     "start_time": "2025-05-16T11:00:44.518326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_prepared_data(file_path):\n",
    "    data = pd.read_csv(file_path,\n",
    "                       low_memory=False)\n",
    "    # Convert 'date' column to datetime object\n",
    "    data['date'] = pd.to_datetime(data['date'], format=\"%Y-%m-%d\", errors='coerce')\n",
    "    data = data.sort_values(by='date')\n",
    "\n",
    "    # Convert today's date to a pandas Timestamp for compatibility.\n",
    "    today = pd.Timestamp(datetime.today().date())\n",
    "    data = data[data['date'] <= today]\n",
    "\n",
    "    # Clean up and finalise the match-level DataFrame\n",
    "    data.dropna(inplace=True)\n",
    "    data['ht_score'] = data['home_goals_ht'].astype(str) + '-' + data['away_goals_ht'].astype(str)\n",
    "    data['total_goals'] = data['home_goals_ft'] + data['away_goals_ft']\n",
    "    data['target'] = ((data['home_goals_ft'] > data['home_goals_ht']) | (\n",
    "                data['away_goals_ft'] > data['away_goals_ht'])).astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "matches = pre_prepared_data(r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\engineered_master_data_ALL_2017+.csv\")\n",
    "matches = pd.get_dummies(matches, columns=['country'], prefix='country')\n",
    "dummy_cols = [col for col in matches.columns if col.startswith('country_')]\n",
    "features = features + dummy_cols\n",
    "# Process each league separately\n",
    "ht_score = matches[['ht_score']].drop_duplicates().apply(tuple, axis=1)"
   ],
   "id": "d7394273217bc600",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:09.511255Z",
     "start_time": "2025-05-16T08:56:09.445780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "matches_filtered = matches[matches['ht_score'] == '1-1']\n",
    "data = matches_filtered.copy()\n",
    "#matches_filtered"
   ],
   "id": "e6ab350b9509433e",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:09.838194Z",
     "start_time": "2025-05-16T08:56:09.817864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def replicate_run_from_csv_row(data, base_features, row):\n",
    "    \"\"\"\n",
    "    Replicates a model run (MLP, XGBoost or RF) from one CSV row,\n",
    "    but first one-hot-encodes 'country' exactly as in your main script.\n",
    "    \"\"\"\n",
    "    # 2) Extract parameters from CSV row\n",
    "    smote_level = row['SMOTE']\n",
    "    threshold = float(row['Probability_Threshold'])\n",
    "    param_dict = ast.literal_eval(row['Params']) if isinstance(row['Params'], str) else row['Params']\n",
    "    model_name = row['Model']\n",
    "\n",
    "    # 3) Train/test split (time-series)\n",
    "    X = data[features]\n",
    "    y = data['target']\n",
    "    split = int(len(data) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    # 4) SMOTE on the training set only\n",
    "    if smote_level not in [None, 'None'] and pd.notna(smote_level):\n",
    "        sm = SMOTE(sampling_strategy=float(smote_level), random_state=42)\n",
    "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "\n",
    "    # 5) Instantiate the correct classifier with your original defaults\n",
    "    if model_name == \"MLP\":\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        clf = MLPClassifier(\n",
    "            random_state=42,\n",
    "            max_iter=10000,  # ← match your pipeline\n",
    "            early_stopping=param_dict.get('classifier__early_stopping', True)\n",
    "        )\n",
    "    elif model_name == \"XGBoost\":\n",
    "        from xgboost import XGBClassifier\n",
    "        clf = XGBClassifier(\n",
    "            random_state=42,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "    elif model_name in [\"Random Forest\", \"RandomForest\"]:\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        clf = RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            class_weight='balanced'  # ← match your pipeline\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # 6) Build and configure the pipeline\n",
    "    pipeline = ImbPipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    pipeline.set_params(**param_dict)\n",
    "\n",
    "    # 7) Fit & predict\n",
    "    pipeline.fit(X_train_res, y_train_res)\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # 8) Compute metrics\n",
    "    return {\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'AUC': roc_auc_score(y_test, y_proba),\n",
    "        'MCC': matthews_corrcoef(y_test, y_pred),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Test_Predicted_Positives': int(y_pred.sum()),\n",
    "        'Confusion_Matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n"
   ],
   "id": "63e697756e8b41a2",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:01:34.287271Z",
     "start_time": "2025-05-16T11:01:32.608858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "row = results_df.iloc[3]\n",
    "metrics = replicate_run_from_csv_row(data, features, row)\n",
    "print(metrics)"
   ],
   "id": "2a811571cbe416ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': np.float64(0.8500881834215167), 'Recall': np.float64(0.33967582804792107), 'F1': np.float64(0.48539778449144005), 'AUC': np.float64(0.5866516059489835), 'MCC': np.float64(0.12641126486429052), 'Accuracy': 0.44516829533116176, 'Test_Predicted_Positives': 567, 'Confusion_Matrix': array([[338,  85],\n",
      "       [937, 482]])}\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:28.360579Z",
     "start_time": "2025-05-16T08:56:11.927586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_save_model(data, base_features, row, output_dir):\n",
    "    \"\"\"\n",
    "    Trains a model for a specific half-time score (ht_score) using the parameters in 'row',\n",
    "    one-hot-encodes 'country', then saves the pipeline + metadata to .pkl if Precision_Test > 0.8.\n",
    "    # \"\"\"\n",
    "    # # 1) One-hot-encode country exactly as in your main pipeline\n",
    "    # if 'country' in data.columns:\n",
    "    #     data = pd.get_dummies(data, columns=['country'], prefix='country')\n",
    "    #     dummy_cols = [c for c in data.columns if c.startswith('country_')]\n",
    "    #     features = base_features + dummy_cols\n",
    "    # else:\n",
    "    #     features = base_features[:]\n",
    "\n",
    "    # 2) Extract parameters\n",
    "    ht_score = row.get('HT_Score', row.get('ht_score', 'unknown'))\n",
    "    smote_level = row['SMOTE']\n",
    "    threshold = float(row['Probability_Threshold'])\n",
    "    precision_test = float(row['Precision_Test'])\n",
    "    param_dict = (ast.literal_eval(row['Params'])\n",
    "                  if isinstance(row['Params'], str)\n",
    "                  else row['Params'])\n",
    "    model_name = row['Model']\n",
    "\n",
    "    # 3) Time-series split (first 80% train)\n",
    "    X = data[features]\n",
    "    y = data['target']\n",
    "    split = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "    # 4) Apply SMOTE only if specified & needed\n",
    "    used_smote = None\n",
    "    if smote_level not in (None, 'None') and pd.notna(smote_level):\n",
    "        desired = float(smote_level)\n",
    "        counts = Counter(y_train)\n",
    "        current = counts[min(counts, key=counts.get)] / counts[max(counts, key=counts.get)]\n",
    "        if desired > current:\n",
    "            sm = SMOTE(sampling_strategy=desired, random_state=42)\n",
    "            X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "            used_smote = desired\n",
    "        else:\n",
    "            print(f\"SMOTE {desired:.2f} ≤ current {current:.2f} for '{ht_score}', skipping SMOTE\")\n",
    "\n",
    "    # 5) Instantiate the classifier\n",
    "    if model_name == \"MLP\":\n",
    "        clf = MLPClassifier(random_state=42, max_iter=10000)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        clf = XGBClassifier(random_state=42,\n",
    "                            eval_metric='logloss')\n",
    "    elif \"RandomForest\" in model_name or \"Random Forest\" in model_name:\n",
    "        clf = RandomForestClassifier(random_state=42,\n",
    "                                     class_weight='balanced')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name!r}\")\n",
    "\n",
    "    # 6) Build and configure pipeline\n",
    "    pipeline = ImbPipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    pipeline.set_params(**param_dict)\n",
    "\n",
    "    # 7) Fit on (possibly resampled) training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # 8) Save if precision_test > 0.8\n",
    "    if precision_test > 0.8:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model_obj = {\n",
    "            'pipeline': pipeline,\n",
    "            'threshold': threshold,\n",
    "            'ht_score': ht_score,\n",
    "            'smote_level': used_smote,\n",
    "            'params': param_dict\n",
    "        }\n",
    "        ht_clean = str(ht_score).replace('/', '-').replace(' ', '')\n",
    "        fname = f\"trained_model_{ht_clean}_thr{threshold:.2f}_sm{(used_smote or 0):.2f}.pkl\"\n",
    "        path = os.path.join(output_dir, fname)\n",
    "        joblib.dump(model_obj, path)\n",
    "        print(f\"Saved model for ht_score '{ht_score}' to: {path}\")\n",
    "        return path\n",
    "\n",
    "    print(f\"Did not save model for ht_score '{ht_score}' (Precision_Test = {precision_test:.2f})\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def train_and_save_all_models(data, features, results_df, output_dir):\n",
    "    \"\"\"\n",
    "    Iterates over each row in results_df (each representing parameters for a league model),\n",
    "    trains the model using those parameters, and saves the model pipeline for future predictions.\n",
    "\n",
    "    Returns a list of file paths of the saved models.\n",
    "    \"\"\"\n",
    "    saved_files = []\n",
    "    for idx, row in results_df.iterrows():\n",
    "        ht_score = row['HT_Score'].strip(\"(',);\")\n",
    "        data = matches[matches['ht_score'] == ht_score]\n",
    "        try:\n",
    "            file_path = train_and_save_model(data, features, row, output_dir)\n",
    "            saved_files.append(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model for row index {idx} (HT Score: {row.get('HT_Score', 'unknown')}): {e}\")\n",
    "    return saved_files\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `data` is your DataFrame containing a 'target' column and the feature columns,\n",
    "# `features` is your list of feature column names, and `results_df` is the DataFrame\n",
    "# with each row containing model parameters for a league model (with 'League', 'SMOTE',\n",
    "# 'Probability_Threshold', 'Params', and 'Model' columns).\n",
    "#\n",
    "output_directory = r\"path_ht_score\\to\\save\\models\"\n",
    "saved_model_files = train_and_save_all_models(data, features, results_df, output_directory)\n",
    "#print(\"Saved model files:\", saved_model_files)\n"
   ],
   "id": "4c48feccaa62e638",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model for ht_score '('0-0',)' to: path_ht_score\\to\\save\\models\\trained_model_('0-0',)_thr0.72_sm0.00.pkl\n",
      "Saved model for ht_score '('0-1',)' to: path_ht_score\\to\\save\\models\\trained_model_('0-1',)_thr0.77_sm0.82.pkl\n",
      "Saved model for ht_score '('1-0',)' to: path_ht_score\\to\\save\\models\\trained_model_('1-0',)_thr0.78_sm0.33.pkl\n",
      "Saved model for ht_score '('1-1',)' to: path_ht_score\\to\\save\\models\\trained_model_('1-1',)_thr0.76_sm0.60.pkl\n",
      "Saved model for ht_score '('1-2',)' to: path_ht_score\\to\\save\\models\\trained_model_('1-2',)_thr0.65_sm0.90.pkl\n",
      "Saved model for ht_score '('2-0',)' to: path_ht_score\\to\\save\\models\\trained_model_('2-0',)_thr0.71_sm0.61.pkl\n",
      "Saved model for ht_score '('2-1',)' to: path_ht_score\\to\\save\\models\\trained_model_('2-1',)_thr0.73_sm0.47.pkl\n",
      "Saved model for ht_score '('3-0',)' to: path_ht_score\\to\\save\\models\\trained_model_('3-0',)_thr0.62_sm0.68.pkl\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:56:28.890625Z",
     "start_time": "2025-05-16T08:56:28.502224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_saved_model(saved_model_path: str,\n",
    "                     data: pd.DataFrame,\n",
    "                     features: list) -> dict:\n",
    "    \"\"\"\n",
    "    Loads a saved model dict (with keys 'pipeline', 'threshold', 'ht_score', 'smote_level'),\n",
    "    then evaluates it on the last 20% of `data` (time-series split) using exactly the\n",
    "    columns in `features`. Returns the same metrics dict you used at training time.\n",
    "    \"\"\"\n",
    "    # 1) Load\n",
    "    model_dict = joblib.load(saved_model_path)\n",
    "    pipeline = model_dict['pipeline']\n",
    "    threshold = float(model_dict['threshold'])\n",
    "    ht_score = model_dict['ht_score']\n",
    "    smote_lvl = model_dict.get('smote_level')\n",
    "\n",
    "    print(f\"Testing ht_score='{ht_score}', threshold={threshold:.2f}, SMOTE={smote_lvl}\")\n",
    "\n",
    "    # 2) Prepare test split (no further encoding—we assume `features` already covers all dummy cols)\n",
    "    X = data[features]\n",
    "    y = data['target']\n",
    "    split = int(len(X) * 0.8)\n",
    "    X_test = X.iloc[split:]\n",
    "    y_test = y.iloc[split:]\n",
    "\n",
    "    # 3) Predict & apply threshold\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # 4) Compute metrics\n",
    "    metrics = {\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'AUC': roc_auc_score(y_test, y_proba),\n",
    "        'MCC': matthews_corrcoef(y_test, y_pred),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Test_Predicted_Positives': int(y_pred.sum()),\n",
    "        'Confusion_Matrix': confusion_matrix(y_test, y_pred),\n",
    "        'SMOTE_Level': smote_lvl\n",
    "    }\n",
    "\n",
    "    print(\"Test metrics:\")\n",
    "    for name, val in metrics.items():\n",
    "        print(f\"  {name}: {val}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def extract_ht_score(model_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a path like\n",
    "      .../trained_model_('2-0',)_thr0.71_sm0.61.pkl\n",
    "    returns the clean ht_score string, e.g. \"2-0\".\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(model_path)\n",
    "    m = re.match(r\"trained_model_(.+?)_thr\", fname)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Filename {fname!r} doesn’t match expected pattern\")\n",
    "    raw = m.group(1)  # e.g. \"('2-0',)\" or \"0-0\"\n",
    "    # if it's a tuple literal, eval and pick the first element\n",
    "    if raw.startswith(\"(\") and raw.endswith(\")\"):\n",
    "        try:\n",
    "            tpl = ast.literal_eval(raw)\n",
    "            return tpl[0]\n",
    "        except Exception:\n",
    "            # fallback to stripping punctuation\n",
    "            return raw.strip(\"()'\\\",\")\n",
    "    else:\n",
    "        return raw\n",
    "\n",
    "\n",
    "# 'features' here already includes your country dummy columns\n",
    "base_features = features\n",
    "\n",
    "# Pick out ht_score from filename\n",
    "model_path = r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\to\\save\\models\\trained_model_('0-0',)_thr0.72_sm0.00.pkl\"\n",
    "ht_score = extract_ht_score(model_path)\n",
    "\n",
    "# Subset your matches DataFrame\n",
    "df_20 = matches[matches['ht_score'] == ht_score]\n",
    "\n",
    "# Run the test\n",
    "results = test_saved_model(model_path, df_20, base_features)\n",
    "results\n"
   ],
   "id": "97ae462ba431b60c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ht_score='('0-0',)', threshold=0.72, SMOTE=None\n",
      "Test metrics:\n",
      "  Precision: 0.8512720156555773\n",
      "  Recall: 0.0825426944971537\n",
      "  F1: 0.15049299429164503\n",
      "  AUC: 0.5870722696256456\n",
      "  MCC: 0.06846761707240423\n",
      "  Accuracy: 0.3065518215193448\n",
      "  Test_Predicted_Positives: 511\n",
      "  Confusion_Matrix: [[1736   76]\n",
      " [4835  435]]\n",
      "  SMOTE_Level: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Precision': np.float64(0.8512720156555773),\n",
       " 'Recall': np.float64(0.0825426944971537),\n",
       " 'F1': np.float64(0.15049299429164503),\n",
       " 'AUC': np.float64(0.5870722696256456),\n",
       " 'MCC': np.float64(0.06846761707240423),\n",
       " 'Accuracy': 0.3065518215193448,\n",
       " 'Test_Predicted_Positives': 511,\n",
       " 'Confusion_Matrix': array([[1736,   76],\n",
       "        [4835,  435]]),\n",
       " 'SMOTE_Level': None}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:33:10.474914Z",
     "start_time": "2025-05-16T11:33:10.452998Z"
    }
   },
   "cell_type": "code",
   "source": "results_df",
   "id": "ed9b33b825c99947",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   HT_Score    Model  SMOTE  Precision_Test  Precision_Test/Train_Ratio  \\\n",
       "0  ('0-0',)  XGBoost    NaN          0.8513                      0.9226   \n",
       "1  ('0-1',)  XGBoost   0.82          0.8525                      0.9492   \n",
       "2  ('1-0',)  XGBoost   0.33          0.8525                      0.9049   \n",
       "3  ('1-1',)  XGBoost   0.60          0.8502                      0.9383   \n",
       "4  ('1-2',)  XGBoost   0.90          0.8531                      0.9380   \n",
       "5  ('2-0',)  XGBoost   0.61          0.8508                      0.9627   \n",
       "6  ('2-1',)  XGBoost   0.47          0.8522                      0.9110   \n",
       "7  ('3-0',)  XGBoost   0.68          0.8528                      0.9066   \n",
       "\n",
       "   Probability_Threshold                                             Params  \n",
       "0                   0.72  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "1                   0.77  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "2                   0.78  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "3                   0.76  {'classifier__colsample_bytree': 0.7, 'classif...  \n",
       "4                   0.65  {'classifier__colsample_bytree': 0.8, 'classif...  \n",
       "5                   0.71  {'classifier__colsample_bytree': 0.8, 'classif...  \n",
       "6                   0.73  {'classifier__colsample_bytree': 0.8, 'classif...  \n",
       "7                   0.62  {'classifier__colsample_bytree': 0.7, 'classif...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HT_Score</th>\n",
       "      <th>Model</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>Precision_Test</th>\n",
       "      <th>Precision_Test/Train_Ratio</th>\n",
       "      <th>Probability_Threshold</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('0-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8513</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>0.72</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('0-1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>0.77</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('1-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>0.9049</td>\n",
       "      <td>0.78</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('1-1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.8502</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>0.76</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>('1-2',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>('2-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.8508</td>\n",
       "      <td>0.9627</td>\n",
       "      <td>0.71</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('2-1',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.9110</td>\n",
       "      <td>0.73</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.8, 'classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('3-0',)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.8528</td>\n",
       "      <td>0.9066</td>\n",
       "      <td>0.62</td>\n",
       "      <td>{'classifier__colsample_bytree': 0.7, 'classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
