{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T11:59:45.012056Z",
     "start_time": "2025-08-18T11:59:45.004264Z"
    }
   },
   "source": [
    "# import os\n",
    "# import re\n",
    "# import glob\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "#\n",
    "# # --- Paths ---\n",
    "# SRC_DIR  = r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\"\n",
    "# DEST_DIR = os.path.join(SRC_DIR, \"best_models_by_ht_scoreline\")\n",
    "# os.makedirs(DEST_DIR, exist_ok=True)\n",
    "#\n",
    "# # --- Filename patterns supported ---\n",
    "# # Singles:\n",
    "# # 1) model_metrics_'0-0'_YYYYMMDD_XXXXXXXX.csv\n",
    "# # 2) model_metrics_('0-0',)_YYYYMMDD_XXXXXXXX.csv\n",
    "# # 3) model_metrics_0-0_YYYYMMDD_XXXXXXXX.csv\n",
    "# # Combined:\n",
    "# # 4) model_metrics_'0-0'_COMBINED_YYYYMMDD_XXXXXXXX.csv\n",
    "# # 5) model_metrics_('0-0',)_COMBINED_YYYYMMDD_XXXXXXXX.csv\n",
    "# # 6) model_metrics_0-0_COMBINED_YYYYMMDD_XXXXXXXX.csv\n",
    "# PATTERNS = [\n",
    "#     re.compile(r\"^model_metrics_'([^']+)'_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "#     re.compile(r\"^model_metrics_\\('([^']+)',\\)_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "#     re.compile(r\"^model_metrics_([^_]+)_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "#     re.compile(r\"^model_metrics_'([^']+)'_COMBINED_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "#     re.compile(r\"^model_metrics_\\('([^']+)',\\)_COMBINED_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "#     re.compile(r\"^model_metrics_([^_]+)_COMBINED_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "# ]\n",
    "#\n",
    "# # Known parameter columns used for de-duplication\n",
    "# XGB_PARAM_COLS = [\n",
    "#     \"n_estimators\", \"max_depth\", \"learning_rate\", \"min_child_weight\",\n",
    "#     \"subsample\", \"colsample_bytree\", \"reg_lambda\",\n",
    "# ]\n",
    "# MLP_PARAM_COLS = [\n",
    "#     \"hidden_layer_sizes\", \"alpha\", \"learning_rate_init\",\n",
    "#     \"batch_size\", \"max_iter\",\n",
    "# ]\n",
    "# DEDUP_KEYS = XGB_PARAM_COLS + MLP_PARAM_COLS + [\"threshold\"]\n",
    "#\n",
    "# # ---------- NEW: delete *_FAILED files ----------\n",
    "# def is_failed_filename(path: str) -> bool:\n",
    "#     base = os.path.basename(path)\n",
    "#     name_no_ext, _ = os.path.splitext(base)\n",
    "#     return base.endswith(\"_FAILED\") or base.endswith(\"_FAILED.csv\") or name_no_ext.endswith(\"_FAILED\")\n",
    "#\n",
    "# def purge_failed_files(dirs: list[str]) -> None:\n",
    "#     for d in dirs:\n",
    "#         pattern = os.path.join(d, \"model_metrics_*\")\n",
    "#         for p in glob.glob(pattern):\n",
    "#             if os.path.isfile(p) and is_failed_filename(p):\n",
    "#                 try:\n",
    "#                     os.remove(p)\n",
    "#                     print(f\"✂ Deleted FAILED file: {p}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[WARN] Could not delete FAILED file {p}: {e}\")\n",
    "# # ------------------------------------------------\n",
    "#\n",
    "# def extract_scoreline(filename: str) -> str | None:\n",
    "#     base = os.path.basename(filename)\n",
    "#     for rx in PATTERNS:\n",
    "#         m = rx.match(base)\n",
    "#         if m:\n",
    "#             return m.group(1)  # scoreline\n",
    "#     return None\n",
    "#\n",
    "# def files_by_scoreline(base_dir: str) -> dict[str, list[str]]:\n",
    "#     out: dict[str, list[str]] = {}\n",
    "#     for path in glob.glob(os.path.join(base_dir, \"model_metrics_*.csv\")):\n",
    "#         # Skip any lingering *_FAILED files defensively\n",
    "#         if is_failed_filename(path):\n",
    "#             continue\n",
    "#         sl = extract_scoreline(os.path.basename(path))\n",
    "#         if not sl:\n",
    "#             continue\n",
    "#         out.setdefault(sl, []).append(path)\n",
    "#     return out\n",
    "#\n",
    "# def read_csv_safely(path: str) -> pd.DataFrame:\n",
    "#     try:\n",
    "#         return pd.read_csv(path, low_memory=False)\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "#\n",
    "# def _coerce_bool_series(s: pd.Series) -> pd.Series:\n",
    "#     if s.dtype == bool:\n",
    "#         return s\n",
    "#     # numeric (1/0) -> True/False\n",
    "#     try:\n",
    "#         sn = pd.to_numeric(s, errors=\"coerce\")\n",
    "#         if sn.notna().any():\n",
    "#             return (sn.fillna(0) != 0)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     # string interpretation\n",
    "#     truthy = {\"true\", \"t\", \"yes\", \"y\", \"1\"}\n",
    "#     vals = s.astype(str).str.strip().str.lower()\n",
    "#     return vals.isin(truthy)\n",
    "#\n",
    "# def filter_by_pass_test_gate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"Keep only rows where pass_test_gate is True (if present).\"\"\"\n",
    "#     if \"pass_test_gate\" not in df.columns:\n",
    "#         return df\n",
    "#     mask = _coerce_bool_series(df[\"pass_test_gate\"])\n",
    "#     return df[mask].copy()\n",
    "#\n",
    "# def sort_and_dedup(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df = filter_by_pass_test_gate(df)\n",
    "#\n",
    "#     for col in [\"test_precision\", \"test_accuracy\", \"n_preds_test\", \"val_precision\"]:\n",
    "#         if col in df.columns:\n",
    "#             df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "#\n",
    "#     if \"n_preds_test\" in df.columns:\n",
    "#         df = df[df[\"n_preds_test\"] >= 100]\n",
    "#\n",
    "#     sort_cols = [c for c in [\"test_precision\", \"test_accuracy\", \"n_preds_test\", \"val_precision\"] if c in df.columns]\n",
    "#     if sort_cols:\n",
    "#         df = df.sort_values(by=sort_cols, ascending=[False] * len(sort_cols))\n",
    "#\n",
    "#     key_cols = [c for c in DEDUP_KEYS if c in df.columns]\n",
    "#     if key_cols:\n",
    "#         df = df.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "#     else:\n",
    "#         df = df.drop_duplicates(keep=\"first\")\n",
    "#\n",
    "#     return df.reset_index(drop=True)\n",
    "#\n",
    "# def combine_for_scoreline(scoreline: str, src_files: list[str], dest_files: list[str]) -> tuple[pd.DataFrame, list[str], list[str]]:\n",
    "#     frames = []\n",
    "#     consumed_src, consumed_dest = [], []\n",
    "#\n",
    "#     # Add source files\n",
    "#     for p in src_files:\n",
    "#         df = read_csv_safely(p)\n",
    "#         if not df.empty:\n",
    "#             df = filter_by_pass_test_gate(df)\n",
    "#             if not df.empty:\n",
    "#                 frames.append(df.assign(_source=p))\n",
    "#                 consumed_src.append(p)\n",
    "#             else:\n",
    "#                 print(f\"  [INFO] All rows failed pass_test_gate in source file: {p}\")\n",
    "#         else:\n",
    "#             print(f\"  [WARN] Empty source file (skipped): {p}\")\n",
    "#\n",
    "#     # Add existing destination files for this scoreline (including prior COMBINEDs)\n",
    "#     for p in dest_files:\n",
    "#         df = read_csv_safely(p)\n",
    "#         if not df.empty:\n",
    "#             df = filter_by_pass_test_gate(df)\n",
    "#             if not df.empty:\n",
    "#                 frames.append(df.assign(_source=p))\n",
    "#                 consumed_dest.append(p)\n",
    "#             else:\n",
    "#                 print(f\"  [INFO] All rows failed pass_test_gate in dest file: {p}\")\n",
    "#         else:\n",
    "#             print(f\"  [WARN] Empty dest file (skipped): {p}\")\n",
    "#\n",
    "#     if not frames:\n",
    "#         return pd.DataFrame(), consumed_src, consumed_dest\n",
    "#\n",
    "#     combined = pd.concat(frames, ignore_index=True, sort=False)\n",
    "#     combined = sort_and_dedup(combined)\n",
    "#     return combined, consumed_src, consumed_dest\n",
    "#\n",
    "# def main():\n",
    "#     # NEW: purge any *_FAILED files up-front in both locations\n",
    "#     purge_failed_files([SRC_DIR, DEST_DIR])\n",
    "#\n",
    "#     # Map scorelines -> files in SRC and DEST (DEST now includes COMBINEDs)\n",
    "#     src_map  = files_by_scoreline(SRC_DIR)\n",
    "#     dest_map = files_by_scoreline(DEST_DIR)\n",
    "#\n",
    "#     all_scorelines = sorted(set(src_map) | set(dest_map))\n",
    "#     if not all_scorelines:\n",
    "#         print(\"No scoreline CSVs found in either directory; nothing to do.\")\n",
    "#         return\n",
    "#\n",
    "#     ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#\n",
    "#     for sl in all_scorelines:\n",
    "#         src_files  = src_map.get(sl, [])\n",
    "#         dest_files = dest_map.get(sl, [])\n",
    "#\n",
    "#         if not src_files and not dest_files:\n",
    "#             continue\n",
    "#\n",
    "#         print(f\"\\n→ Processing scoreline: {sl} (src: {len(src_files)}, dest: {len(dest_files)})\")\n",
    "#         combined_df, consumed_src, consumed_dest = combine_for_scoreline(sl, src_files, dest_files)\n",
    "#\n",
    "#         if combined_df.empty:\n",
    "#             print(f\"  [WARN] Nothing to combine for {sl}.\")\n",
    "#             continue\n",
    "#\n",
    "#         # Write a fresh combined file to DEST\n",
    "#         out_name = f\"model_metrics_'{sl}'_COMBINED_{ts}.csv\"\n",
    "#         out_path = os.path.join(DEST_DIR, out_name)\n",
    "#         combined_df.drop(columns=[c for c in [\"_source\"] if c in combined_df.columns], inplace=True, errors=\"ignore\")\n",
    "#         combined_df.to_csv(out_path, index=False)\n",
    "#         print(f\"  ✓ Wrote combined file with {len(combined_df)} rows:\\n    {out_path}\")\n",
    "#\n",
    "#         # Delete originals merged FROM SRC\n",
    "#         for p in consumed_src:\n",
    "#             try:\n",
    "#                 os.remove(p)\n",
    "#                 print(f\"  ✂ Deleted source file: {p}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  [WARN] Could not delete source file {p}: {e}\")\n",
    "#\n",
    "#         # Delete originals merged FROM DEST (older combineds/singles)\n",
    "#         for p in consumed_dest:\n",
    "#             if os.path.abspath(p) == os.path.abspath(out_path):\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 os.remove(p)\n",
    "#                 print(f\"  ✂ Deleted old dest file: {p}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  [WARN] Could not delete dest file {p}: {e}\")\n",
    "#\n",
    "#     print(\"\\nAll done.\")\n",
    "#\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T11:59:45.890963Z",
     "start_time": "2025-08-18T11:59:45.167315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# --- Paths ---\n",
    "SRC_DIR  = r\"C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\"\n",
    "DEST_DIR = os.path.join(SRC_DIR, \"best_models_by_ht_scoreline\")\n",
    "PATH_HT_SCORE = os.path.join(SRC_DIR, \"path_ht_score\")\n",
    "os.makedirs(DEST_DIR, exist_ok=True)\n",
    "\n",
    "# Safety for model-file pruning: set to False to actually delete non-best .pkl files\n",
    "DRY_RUN_PKLS = False\n",
    "\n",
    "# --- Filename patterns supported ---\n",
    "PATTERNS = [\n",
    "    re.compile(r\"^model_metrics_'([^']+)'_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "    re.compile(r\"^model_metrics_\\('([^']+)',\\)_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "    re.compile(r\"^model_metrics_([^_]+)_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "    re.compile(r\"^model_metrics_'([^']+)'_COMBINED_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "    re.compile(r\"^model_metrics_\\('([^']+)',\\)_COMBINED_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "    re.compile(r\"^model_metrics_([^_]+)_COMBINED_(\\d{8})_(\\d{6})\\.csv$\"),\n",
    "]\n",
    "\n",
    "# Known parameter columns used for de-duplication\n",
    "XGB_PARAM_COLS = [\n",
    "    \"n_estimators\", \"max_depth\", \"learning_rate\", \"min_child_weight\",\n",
    "    \"subsample\", \"colsample_bytree\", \"reg_lambda\",\n",
    "]\n",
    "MLP_PARAM_COLS = [\n",
    "    \"hidden_layer_sizes\", \"alpha\", \"learning_rate_init\",\n",
    "    \"batch_size\", \"max_iter\",\n",
    "]\n",
    "DEDUP_KEYS = XGB_PARAM_COLS + MLP_PARAM_COLS + [\"threshold\"]\n",
    "\n",
    "# ---------- delete *_FAILED files ----------\n",
    "def is_failed_filename(path: str) -> bool:\n",
    "    base = os.path.basename(path)\n",
    "    name_no_ext, _ = os.path.splitext(base)\n",
    "    return base.endswith(\"_FAILED\") or base.endswith(\"_FAILED.csv\") or name_no_ext.endswith(\"_FAILED\")\n",
    "\n",
    "def purge_failed_files(dirs: list[str]) -> None:\n",
    "    for d in dirs:\n",
    "        pattern = os.path.join(d, \"model_metrics_*\")\n",
    "        for p in glob.glob(pattern):\n",
    "            if os.path.isfile(p) and is_failed_filename(p):\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                    print(f\"✂ Deleted FAILED file: {p}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Could not delete FAILED file {p}: {e}\")\n",
    "# ------------------------------------------------\n",
    "\n",
    "def extract_scoreline(filename: str) -> str | None:\n",
    "    base = os.path.basename(filename)\n",
    "    for rx in PATTERNS:\n",
    "        m = rx.match(base)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "    return None\n",
    "\n",
    "def files_by_scoreline(base_dir: str) -> dict[str, list[str]]:\n",
    "    out: dict[str, list[str]] = {}\n",
    "    for path in glob.glob(os.path.join(base_dir, \"model_metrics_*.csv\")):\n",
    "        if is_failed_filename(path):\n",
    "            continue\n",
    "        sl = extract_scoreline(os.path.basename(path))\n",
    "        if not sl:\n",
    "            continue\n",
    "        out.setdefault(sl, []).append(path)\n",
    "    return out\n",
    "\n",
    "def read_csv_safely(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _coerce_bool_series(s: pd.Series) -> pd.Series:\n",
    "    if s.dtype == bool:\n",
    "        return s\n",
    "    try:\n",
    "        sn = pd.to_numeric(s, errors=\"coerce\")\n",
    "        if sn.notna().any():\n",
    "            return (sn.fillna(0) != 0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    truthy = {\"true\", \"t\", \"yes\", \"y\", \"1\"}\n",
    "    vals = s.astype(str).str.strip().str.lower()\n",
    "    return vals.isin(truthy)\n",
    "\n",
    "def filter_by_pass_test_gate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"pass_test_gate\" not in df.columns:\n",
    "        return df\n",
    "    mask = _coerce_bool_series(df[\"pass_test_gate\"])\n",
    "    return df[mask].copy()\n",
    "\n",
    "def sort_and_dedup(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = filter_by_pass_test_gate(df)\n",
    "\n",
    "    # Coerce numeric for sorting metrics\n",
    "    numeric_cols = [\"val_precision_lcb\", \"val_precision\", \"test_precision\", \"test_accuracy\", \"n_preds_test\"]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Optional guard\n",
    "    if \"n_preds_test\" in df.columns:\n",
    "        df = df[df[\"n_preds_test\"] >= 100]\n",
    "\n",
    "    # Primary sort: val_precision_lcb (desc) with fallbacks if present\n",
    "    sort_cols = [c for c in [\"val_precision_lcb\", \"val_precision\", \"test_precision\", \"test_accuracy\", \"n_preds_test\"] if c in df.columns]\n",
    "    if sort_cols:\n",
    "        df = df.sort_values(by=sort_cols, ascending=[False] * len(sort_cols))\n",
    "\n",
    "    # De-duplicate on hyper-parameters (if available)\n",
    "    key_cols = [c for c in DEDUP_KEYS if c in df.columns]\n",
    "    if key_cols:\n",
    "        df = df.drop_duplicates(subset=key_cols, keep=\"first\")\n",
    "    else:\n",
    "        df = df.drop_duplicates(keep=\"first\")\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def combine_for_scoreline(scoreline: str, src_files: list[str], dest_files: list[str]) -> tuple[pd.DataFrame, list[str], list[str]]:\n",
    "    frames = []\n",
    "    consumed_src, consumed_dest = [], []\n",
    "\n",
    "    # Ingest source files\n",
    "    for p in src_files:\n",
    "        df = read_csv_safely(p)\n",
    "        if not df.empty:\n",
    "            df = filter_by_pass_test_gate(df)\n",
    "            if not df.empty:\n",
    "                frames.append(df.assign(_source=p))\n",
    "                consumed_src.append(p)\n",
    "            else:\n",
    "                print(f\"  [INFO] All rows failed pass_test_gate in source file: {p}\")\n",
    "        else:\n",
    "            print(f\"  [WARN] Empty source file (skipped): {p}\")\n",
    "\n",
    "    # Ingest existing dest files (older COMBINEDs/singles)\n",
    "    for p in dest_files:\n",
    "        df = read_csv_safely(p)\n",
    "        if not df.empty:\n",
    "            df = filter_by_pass_test_gate(df)\n",
    "            if not df.empty:\n",
    "                frames.append(df.assign(_source=p))\n",
    "                consumed_dest.append(p)\n",
    "            else:\n",
    "                print(f\"  [INFO] All rows failed pass_test_gate in dest file: {p}\")\n",
    "        else:\n",
    "            print(f\"  [WARN] Empty dest file (skipped): {p}\")\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(), consumed_src, consumed_dest\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    combined = sort_and_dedup(combined)\n",
    "    return combined, consumed_src, consumed_dest\n",
    "\n",
    "# ---------- gather best model_pkl from COMBINEDs ----------\n",
    "def _norm_path(p: str) -> str | None:\n",
    "    if not isinstance(p, str) or not p.strip():\n",
    "        return None\n",
    "    p = p.strip().strip('\"').strip(\"'\")\n",
    "    if not os.path.isabs(p):\n",
    "        p = os.path.join(PATH_HT_SCORE, p)\n",
    "    try:\n",
    "        return os.path.realpath(p)\n",
    "    except Exception:\n",
    "        return os.path.abspath(p)\n",
    "\n",
    "def collect_best_model_pkls_from_combined() -> set[str]:\n",
    "    keep = set()\n",
    "    combined_glob = os.path.join(DEST_DIR, \"model_metrics_*_COMBINED_*.csv\")\n",
    "    files = sorted(glob.glob(combined_glob))\n",
    "    if not files:\n",
    "        print(\"[INFO] No COMBINED files found to collate best models from.\")\n",
    "        return keep\n",
    "\n",
    "    for f in files:\n",
    "        df = read_csv_safely(f)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        # Ensure current sorting rule is applied\n",
    "        df = sort_and_dedup(df)\n",
    "\n",
    "        if \"model_pkl\" not in df.columns or df.empty:\n",
    "            if \"model_pkl\" not in df.columns:\n",
    "                print(f\"  [WARN] 'model_pkl' column missing in {f}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        best_path_raw = str(df.iloc[0][\"model_pkl\"])\n",
    "        np = _norm_path(best_path_raw)\n",
    "        if np:\n",
    "            keep.add(np)\n",
    "            print(f\"  ✓ Best model from {os.path.basename(f)} -> {np}\")\n",
    "        else:\n",
    "            print(f\"  [WARN] Could not normalise model path from {f}: {best_path_raw}\")\n",
    "\n",
    "    return keep\n",
    "\n",
    "# ---------- prune PATH_HT_SCORE to only keep best ----------\n",
    "def prune_path_ht_score(keep_paths: set[str]) -> None:\n",
    "    if not os.path.isdir(PATH_HT_SCORE):\n",
    "        print(f\"[WARN] PATH_HT_SCORE does not exist: {PATH_HT_SCORE}\")\n",
    "        return\n",
    "\n",
    "    all_files = [os.path.realpath(os.path.join(PATH_HT_SCORE, name))\n",
    "                 for name in os.listdir(PATH_HT_SCORE)\n",
    "                 if os.path.isfile(os.path.join(PATH_HT_SCORE, name))]\n",
    "\n",
    "    all_pkls = [p for p in all_files if p.lower().endswith(\".pkl\")]\n",
    "    keep_pkls = {p for p in keep_paths if p.lower().endswith(\".pkl\")}\n",
    "\n",
    "    to_delete = sorted(set(all_pkls) - keep_pkls)\n",
    "\n",
    "    print(\"\\n— Pruning path_ht_score —\")\n",
    "    print(f\" Found .pkl files: {len(all_pkls)}\")\n",
    "    print(f\" Best .pkl (keep): {len(keep_pkls)}\")\n",
    "    print(f\" Will delete: {len(to_delete)}\")\n",
    "\n",
    "    if DRY_RUN_PKLS:\n",
    "        for p in to_delete[:20]:\n",
    "            print(f\"   (dry-run) would delete: {p}\")\n",
    "        if len(to_delete) > 20:\n",
    "            print(f\"   ...and {len(to_delete) - 20} more.\")\n",
    "        print(\" Set DRY_RUN_PKLS = False to actually delete.\")\n",
    "        return\n",
    "\n",
    "    for p in to_delete:\n",
    "        try:\n",
    "            os.remove(p)\n",
    "            print(f\" ✂ Deleted: {p}\")\n",
    "        except Exception as e:\n",
    "            print(f\" [WARN] Could not delete {p}: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # 1) Clean up *_FAILED CSVs first\n",
    "    purge_failed_files([SRC_DIR, DEST_DIR])\n",
    "\n",
    "    # 2) Map files by scoreline (SRC and DEST), so we can combine\n",
    "    src_map  = files_by_scoreline(SRC_DIR)\n",
    "    dest_map = files_by_scoreline(DEST_DIR)\n",
    "\n",
    "    all_scorelines = sorted(set(src_map) | set(dest_map))\n",
    "    if not all_scorelines:\n",
    "        print(\"No scoreline CSVs found in either directory; nothing to do.\")\n",
    "    else:\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        for sl in all_scorelines:\n",
    "            src_files  = src_map.get(sl, [])\n",
    "            dest_files = dest_map.get(sl, [])\n",
    "\n",
    "            if not src_files and not dest_files:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n→ Processing scoreline: {sl} (src: {len(src_files)}, dest: {len(dest_files)})\")\n",
    "            combined_df, consumed_src, consumed_dest = combine_for_scoreline(sl, src_files, dest_files)\n",
    "\n",
    "            if combined_df.empty:\n",
    "                print(f\"  [WARN] Nothing to combine for {sl}.\")\n",
    "                # If nothing combined, still remove any empty/failed leftovers from SRC only\n",
    "                continue\n",
    "\n",
    "            # Write fresh COMBINED to DEST\n",
    "            out_name = f\"model_metrics_'{sl}'_COMBINED_{ts}.csv\"\n",
    "            out_path = os.path.join(DEST_DIR, out_name)\n",
    "            combined_df.drop(columns=[c for c in [\"_source\"] if c in combined_df.columns], inplace=True, errors=\"ignore\")\n",
    "            combined_df.to_csv(out_path, index=False)\n",
    "            print(f\"  ✓ Wrote combined file with {len(combined_df)} rows:\\n    {out_path}\")\n",
    "\n",
    "            # Delete originals merged FROM SRC\n",
    "            for p in consumed_src:\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                    print(f\"  ✂ Deleted source file: {p}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  [WARN] Could not delete source file {p}: {e}\")\n",
    "\n",
    "            # Delete originals merged FROM DEST (older COMBINEDs/singles), but never the brand-new file\n",
    "            for p in consumed_dest:\n",
    "                if os.path.abspath(p) == os.path.abspath(out_path):\n",
    "                    continue\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                    print(f\"  ✂ Deleted old dest file: {p}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  [WARN] Could not delete dest file {p}: {e}\")\n",
    "\n",
    "    # 3) Collate best model files (top-row 'model_pkl') from the remaining COMBINEDs (the fresh ones)\n",
    "    keep_best_pkls = collect_best_model_pkls_from_combined()\n",
    "\n",
    "    # 4) Prune path_ht_score to only those best models\n",
    "    prune_path_ht_score(keep_best_pkls)\n",
    "\n",
    "    print(\"\\nAll done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "6ef8ffd7abe975ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂ Deleted FAILED file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'2-0'_20250818_105249_FAILED.csv\n",
      "✂ Deleted FAILED file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'3-0'_20250818_103925_FAILED.csv\n",
      "\n",
      "→ Processing scoreline: 0-0 (src: 1, dest: 1)\n",
      "  ✓ Wrote combined file with 50 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'0-0'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted source file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'0-0'_20250818_094805.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'0-0'_COMBINED_20250816_113140.csv\n",
      "\n",
      "→ Processing scoreline: 0-1 (src: 1, dest: 1)\n",
      "  ✓ Wrote combined file with 41 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'0-1'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted source file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'0-1'_20250818_101226.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'0-1'_COMBINED_20250816_113140.csv\n",
      "\n",
      "→ Processing scoreline: 0-2 (src: 1, dest: 1)\n",
      "  ✓ Wrote combined file with 29 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'0-2'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted source file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'0-2'_20250818_110230.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'0-2'_COMBINED_20250816_113140.csv\n",
      "\n",
      "→ Processing scoreline: 1-0 (src: 1, dest: 1)\n",
      "  ✓ Wrote combined file with 45 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'1-0'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted source file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'1-0'_20250818_100141.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'1-0'_COMBINED_20250816_113140.csv\n",
      "\n",
      "→ Processing scoreline: 1-1 (src: 1, dest: 1)\n",
      "  ✓ Wrote combined file with 32 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'1-1'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted source file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'1-1'_20250818_102319.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'1-1'_COMBINED_20250816_113140.csv\n",
      "\n",
      "→ Processing scoreline: 1-2 (src: 1, dest: 1)\n",
      "  ✓ Wrote combined file with 24 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'1-2'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted source file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'1-2'_20250818_103039.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'1-2'_COMBINED_20250816_113140.csv\n",
      "\n",
      "→ Processing scoreline: 2-0 (src: 0, dest: 1)\n",
      "  ✓ Wrote combined file with 17 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'2-0'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'2-0'_COMBINED_20250816_113140.csv\n",
      "\n",
      "→ Processing scoreline: 2-1 (src: 1, dest: 1)\n",
      "  ✓ Wrote combined file with 13 rows:\n",
      "    C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'2-1'_COMBINED_20250818_125945.csv\n",
      "  ✂ Deleted source file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\model_metrics_'2-1'_20250818_111536.csv\n",
      "  ✂ Deleted old dest file: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\best_models_by_ht_scoreline\\model_metrics_'2-1'_COMBINED_20250816_113140.csv\n",
      "  ✓ Best model from model_metrics_'0-0'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'0-0'_xgb_calibrated_20250816_105804.pkl\n",
      "  ✓ Best model from model_metrics_'0-1'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'0-1'_xgb_calibrated_20250818_101226.pkl\n",
      "  ✓ Best model from model_metrics_'0-2'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'0-2'_xgb_calibrated_20250818_110230.pkl\n",
      "  ✓ Best model from model_metrics_'1-0'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'1-0'_xgb_calibrated_20250816_110416.pkl\n",
      "  ✓ Best model from model_metrics_'1-1'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'1-1'_xgb_calibrated_20250818_102319.pkl\n",
      "  ✓ Best model from model_metrics_'1-2'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'1-2'_xgb_calibrated_20250816_103819.pkl\n",
      "  ✓ Best model from model_metrics_'2-0'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'2-0'_xgb_calibrated_20250816_112224.pkl\n",
      "  ✓ Best model from model_metrics_'2-1'_COMBINED_20250818_125945.csv -> C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'2-1'_xgb_calibrated_20250818_111536.pkl\n",
      "\n",
      "— Pruning path_ht_score —\n",
      " Found .pkl files: 15\n",
      " Best .pkl (keep): 8\n",
      " Will delete: 7\n",
      " ✂ Deleted: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'0-0'_xgb_calibrated_20250818_094805.pkl\n",
      " ✂ Deleted: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'0-1'_xgb_calibrated_20250816_111003.pkl\n",
      " ✂ Deleted: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'0-2'_xgb_calibrated_20250816_112517.pkl\n",
      " ✂ Deleted: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'1-0'_xgb_calibrated_20250818_100141.pkl\n",
      " ✂ Deleted: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'1-1'_xgb_calibrated_20250816_103801.pkl\n",
      " ✂ Deleted: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'1-2'_xgb_calibrated_20250818_103039.pkl\n",
      " ✂ Deleted: C:\\Users\\leere\\PycharmProjects\\Football_ML3\\Goals\\2H_goal\\ht_scoreline\\path_ht_score\\best_model_'2-1'_xgb_calibrated_20250816_103942.pkl\n",
      "\n",
      "All done.\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
